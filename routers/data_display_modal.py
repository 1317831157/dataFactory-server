"""
æ•°æ®å±•ç¤ºæ¨¡æ€æ¡†APIè·¯ç”±
ä¸ºDataDisplayModalç»„ä»¶æä¾›åç«¯æ¥å£æ”¯æŒ
ä½¿ç”¨get_auto_analysis_resultä½œä¸ºä¸»è¦æ•°æ®æº
"""

from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
import logging
from services.resource_service import ResourceService

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/data-sources", tags=["data-display"])

# ==================== æ•°æ®æ¨¡å‹å®šä¹‰ ====================

class DataSourceOption(BaseModel):
    label: str
    value: str

class MetricData(BaseModel):
    title: str
    value: str
    trend: str

class LatestDataItem(BaseModel):
    title: str
    type: str
    dataSize: int
    updateDate: str

class ChartData(BaseModel):
    categories: List[str]
    values: List[int]

class DataSourceConfigResponse(BaseModel):
    dataSourceOptions: List[DataSourceOption]
    fieldOptions: List[str]
    subTypeOptions: Dict[str, List[str]]
    exportOptions: List[str]

class DataStatsResponse(BaseModel):
    metricData: List[MetricData]
    chartData: ChartData
    latestData: List[LatestDataItem]

class DataUpdateParams(BaseModel):
    dataSource: str
    selectedSubTypes: List[str]
    dateRange: List[str]  # [start_date, end_date]
    selectedFields: List[str]

class DataExportParams(BaseModel):
    format: str
    dataSource: Optional[str] = None
    dateRange: Optional[List[str]] = None
    fields: Optional[List[str]] = None

# ==================== é…ç½®æ•°æ® ====================

# æ•°æ®æºé€‰é¡¹é…ç½®
DATA_SOURCE_OPTIONS = [
    {"label": "ğŸ“š å­¦æœ¯è®ºæ–‡", "value": "academic_papers"},
    {"label": "ğŸ“Š è°ƒæŸ¥æŠ¥å‘Š", "value": "survey_reports"},
    {"label": "ğŸ“– ä¸“ä¸šä¹¦ç±", "value": "professional_books"},
    {"label": "ğŸ“œ æ”¿ç­–æ–‡ä»¶", "value": "policy_documents"},
    {"label": "âš–ï¸ æ³•è§„æ ‡å‡†", "value": "regulations"},
]

# ç ”ç©¶é¢†åŸŸé€‰é¡¹
FIELD_OPTIONS = [
    "è®¡ç®—æœºç§‘å­¦", "äººå·¥æ™ºèƒ½", "æ•°æ®ç§‘å­¦", "æœºå™¨å­¦ä¹ ",
    "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†", "è®¡ç®—æœºè§†è§‰", "çŸ¥è¯†å›¾è°±"
]

# å­ç±»å‹é€‰é¡¹æ˜ å°„
SUB_TYPE_OPTIONS = {
    "academic_papers": ["æœŸåˆŠè®ºæ–‡", "ä¼šè®®è®ºæ–‡", "å­¦ä½è®ºæ–‡", "é¢„å°æœ¬"],
    "survey_reports": ["è¡Œä¸šæŠ¥å‘Š", "å¸‚åœºè°ƒç ”", "ç”¨æˆ·ç ”ç©¶", "æŠ€æœ¯è¯„ä¼°"],
    "professional_books": ["æ•™æ", "ä¸“è‘—", "å‚è€ƒä¹¦", "æŠ€æœ¯æ‰‹å†Œ"],
    "policy_documents": ["å›½å®¶æ”¿ç­–", "è¡Œä¸šæ”¿ç­–", "åœ°æ–¹æ”¿ç­–", "å›½é™…æ”¿ç­–"],
    "regulations": ["å›½å®¶æ ‡å‡†", "è¡Œä¸šæ ‡å‡†", "ä¼ä¸šæ ‡å‡†", "å›½é™…æ ‡å‡†"]
}

# å¯¼å‡ºæ ¼å¼é€‰é¡¹
EXPORT_OPTIONS = ["CSV", "Excel", "PDF", "JSON"]

# ==================== è¾…åŠ©å‡½æ•° ====================

def map_resource_to_data_type(resource_name: str) -> str:
    """å°†èµ„æºåç§°æ˜ å°„åˆ°æ•°æ®ç±»å‹"""
    name_lower = resource_name.lower()
    
    if any(keyword in name_lower for keyword in ["è®ºæ–‡", "paper", "å­¦æœ¯", "ç ”ç©¶"]):
        return "å­¦æœ¯è®ºæ–‡"
    elif any(keyword in name_lower for keyword in ["æŠ¥å‘Š", "report", "è°ƒç ”", "åˆ†æ"]):
        return "è°ƒæŸ¥æŠ¥å‘Š"
    elif any(keyword in name_lower for keyword in ["ä¹¦ç±", "book", "æ•™æ", "ä¸“è‘—"]):
        return "ä¸“ä¸šä¹¦ç±"
    elif any(keyword in name_lower for keyword in ["æ”¿ç­–", "policy", "è§„å®š", "åˆ¶åº¦"]):
        return "æ”¿ç­–æ–‡ä»¶"
    elif any(keyword in name_lower for keyword in ["æ³•è§„", "æ ‡å‡†", "è§„èŒƒ", "æ¡ä¾‹"]):
        return "æ³•è§„æ ‡å‡†"
    else:
        return "å­¦æœ¯è®ºæ–‡"  # é»˜è®¤ç±»å‹

async def get_auto_analysis_data():
    """è·å–è‡ªåŠ¨åˆ†ææ•°æ®"""
    try:
        return await ResourceService.get_auto_analysis_result()
    except Exception as e:
        logger.error(f"Failed to get auto analysis result: {e}")
        return None

# ==================== APIæ¥å£ ====================

@router.get("/config")
async def get_data_source_config():
    """è·å–æ•°æ®æºé…ç½®ä¿¡æ¯"""
    try:
        return {
            "code": 200,
            "message": "Success",
            "data": {
                "dataSourceOptions": DATA_SOURCE_OPTIONS,
                "fieldOptions": FIELD_OPTIONS,
                "subTypeOptions": SUB_TYPE_OPTIONS,
                "exportOptions": EXPORT_OPTIONS
            }
        }
    except Exception as e:
        logger.error(f"Error getting data source config: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{data_source}/sub-types")
async def get_sub_type_options(data_source: str):
    """æ ¹æ®æ•°æ®æºç±»å‹è·å–å­ç±»å‹é€‰é¡¹"""
    try:
        sub_types = SUB_TYPE_OPTIONS.get(data_source, [])
        return {
            "code": 200,
            "message": "Success",
            "data": {"subTypes": sub_types}
        }
    except Exception as e:
        logger.error(f"Error getting sub type options: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/stats")
async def get_data_stats(
    dataSource: Optional[str] = Query(None),
    dateRange: Optional[str] = Query(None),
    fields: Optional[str] = Query(None)
):
    """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨get_auto_analysis_resultä½œä¸ºæ•°æ®æº"""
    try:
        # è·å–è‡ªåŠ¨åˆ†æç»“æœ
        auto_analysis_data = await get_auto_analysis_data()
        
        if auto_analysis_data:
            # åŸºäºçœŸå®æ•°æ®ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            total_count = sum(item.count for item in auto_analysis_data)
            
            # ç”ŸæˆæŒ‡æ ‡æ•°æ®
            metric_data = [
                {"title": "æ–‡çŒ®æ€»é‡", "value": f"{total_count:,}", "trend": "+12.5%"},
                {"title": "å›¾æ–‡æ•°æ®é›†", "value": f"{int(total_count * 0.3):,}", "trend": "+8.3%"},
                {"title": "æ•°æ®è¦†ç›–ç‡", "value": "25.8%", "trend": "+2.1%"},
                {"title": "æ•°æ®æºç±»å‹", "value": str(len(auto_analysis_data)), "trend": "+1"}
            ]
            
            # ç”Ÿæˆå›¾è¡¨æ•°æ®
            categories = [item.name for item in auto_analysis_data[:5]]  # å–å‰5ä¸ª
            values = [item.count for item in auto_analysis_data[:5]]
            
            # ç”Ÿæˆæœ€æ–°æ•°æ®åˆ—è¡¨
            latest_data = []
            for i, item in enumerate(auto_analysis_data[:5]):
                data_type = map_resource_to_data_type(item.name)
                latest_data.append({
                    "title": item.name,
                    "type": data_type,
                    "dataSize": item.count,
                    "updateDate": (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
                })
        else:
            # ä½¿ç”¨é»˜è®¤æ•°æ®
            metric_data = [
                {"title": "æ–‡çŒ®æ€»é‡", "value": "52,489", "trend": "+12.5%"},
                {"title": "å›¾æ–‡æ•°æ®é›†", "value": "15,932", "trend": "+8.3%"},
                {"title": "æ•°æ®è¦†ç›–ç‡", "value": "25.8%", "trend": "+2.1%"},
                {"title": "æ•°æ®æºç±»å‹", "value": "8", "trend": "+1"}
            ]
            
            categories = ["å­¦æœ¯è®ºæ–‡", "è°ƒæŸ¥æŠ¥å‘Š", "ä¸“ä¸šä¹¦ç±", "æ”¿ç­–æ–‡ä»¶", "æ³•è§„æ ‡å‡†"]
            values = [25000, 12000, 8000, 4500, 3000]
            
            latest_data = [
                {"title": "äººå·¥æ™ºèƒ½å‘å±•ç™½çš®ä¹¦", "type": "è°ƒæŸ¥æŠ¥å‘Š", "dataSize": 1250, "updateDate": "2024-01-15"},
                {"title": "æ•°æ®å®‰å…¨æ ‡å‡†è§„èŒƒ", "type": "æ³•è§„æ ‡å‡†", "dataSize": 856, "updateDate": "2024-01-14"},
                {"title": "æœºå™¨å­¦ä¹ æœ€æ–°è¿›å±•", "type": "å­¦æœ¯è®ºæ–‡", "dataSize": 2103, "updateDate": "2024-01-13"},
                {"title": "çŸ¥è¯†å›¾è°±åº”ç”¨ç ”ç©¶", "type": "å­¦æœ¯è®ºæ–‡", "dataSize": 1587, "updateDate": "2024-01-12"},
                {"title": "è¡Œä¸šæ•°å­—åŒ–è½¬å‹æŠ¥å‘Š", "type": "è°ƒæŸ¥æŠ¥å‘Š", "dataSize": 945, "updateDate": "2024-01-10"}
            ]
        
        return {
            "code": 200,
            "message": "Success",
            "data": {
                "metricData": metric_data,
                "chartData": {"categories": categories, "values": values},
                "latestData": latest_data
            }
        }
    except Exception as e:
        logger.error(f"Error getting data stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/metrics")
async def get_metric_data():
    """è·å–æŒ‡æ ‡å¡æ•°æ®"""
    try:
        auto_analysis_data = await get_auto_analysis_data()

        if auto_analysis_data:
            total_count = sum(item.count for item in auto_analysis_data)
            metrics = [
                {"title": "æ–‡çŒ®æ€»é‡", "value": f"{total_count:,}", "trend": "+12.5%"},
                {"title": "å›¾æ–‡æ•°æ®é›†", "value": f"{int(total_count * 0.3):,}", "trend": "+8.3%"},
                {"title": "æ•°æ®è¦†ç›–ç‡", "value": "25.8%", "trend": "+2.1%"},
                {"title": "æ•°æ®æºç±»å‹", "value": str(len(auto_analysis_data)), "trend": "+1"}
            ]
        else:
            metrics = [
                {"title": "æ–‡çŒ®æ€»é‡", "value": "52,489", "trend": "+12.5%"},
                {"title": "å›¾æ–‡æ•°æ®é›†", "value": "15,932", "trend": "+8.3%"},
                {"title": "æ•°æ®è¦†ç›–ç‡", "value": "25.8%", "trend": "+2.1%"},
                {"title": "æ•°æ®æºç±»å‹", "value": "8", "trend": "+1"}
            ]

        return {
            "code": 200,
            "message": "Success",
            "data": {"metrics": metrics}
        }
    except Exception as e:
        logger.error(f"Error getting metric data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/chart-data")
async def get_chart_data(
    dataSource: Optional[str] = Query(None),
    dateRange: Optional[str] = Query(None)
):
    """è·å–å›¾è¡¨æ•°æ®"""
    try:
        # TODO: æ ¹æ®dataSourceå’ŒdateRangeå‚æ•°ç­›é€‰æ•°æ®
        logger.info(f"Getting chart data for dataSource: {dataSource}, dateRange: {dateRange}")

        auto_analysis_data = await get_auto_analysis_data()

        if auto_analysis_data:
            categories = [item.name for item in auto_analysis_data[:5]]
            values = [item.count for item in auto_analysis_data[:5]]
        else:
            categories = ["å­¦æœ¯è®ºæ–‡", "è°ƒæŸ¥æŠ¥å‘Š", "ä¸“ä¸šä¹¦ç±", "æ”¿ç­–æ–‡ä»¶", "æ³•è§„æ ‡å‡†"]
            values = [25000, 12000, 8000, 4500, 3000]

        return {
            "code": 200,
            "message": "Success",
            "data": {"categories": categories, "values": values}
        }
    except Exception as e:
        logger.error(f"Error getting chart data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/latest")
async def get_latest_data(
    limit: Optional[int] = Query(5),
    dataSource: Optional[str] = Query(None),
    type: Optional[str] = Query(None)
):
    """è·å–æœ€æ–°æ•°æ®åˆ—è¡¨"""
    try:
        # TODO: æ ¹æ®dataSourceå’Œtypeå‚æ•°ç­›é€‰æ•°æ®
        logger.info(f"Getting latest data with limit: {limit}, dataSource: {dataSource}, type: {type}")

        auto_analysis_data = await get_auto_analysis_data()

        if auto_analysis_data:
            latest_data = []
            for i, item in enumerate(auto_analysis_data[:limit]):
                data_type = map_resource_to_data_type(item.name)
                latest_data.append({
                    "title": item.name,
                    "type": data_type,
                    "dataSize": item.count,
                    "updateDate": (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
                })
        else:
            latest_data = [
                {"title": "äººå·¥æ™ºèƒ½å‘å±•ç™½çš®ä¹¦", "type": "è°ƒæŸ¥æŠ¥å‘Š", "dataSize": 1250, "updateDate": "2024-01-15"},
                {"title": "æ•°æ®å®‰å…¨æ ‡å‡†è§„èŒƒ", "type": "æ³•è§„æ ‡å‡†", "dataSize": 856, "updateDate": "2024-01-14"},
                {"title": "æœºå™¨å­¦ä¹ æœ€æ–°è¿›å±•", "type": "å­¦æœ¯è®ºæ–‡", "dataSize": 2103, "updateDate": "2024-01-13"},
                {"title": "çŸ¥è¯†å›¾è°±åº”ç”¨ç ”ç©¶", "type": "å­¦æœ¯è®ºæ–‡", "dataSize": 1587, "updateDate": "2024-01-12"},
                {"title": "è¡Œä¸šæ•°å­—åŒ–è½¬å‹æŠ¥å‘Š", "type": "è°ƒæŸ¥æŠ¥å‘Š", "dataSize": 945, "updateDate": "2024-01-10"}
            ][:limit]

        return {
            "code": 200,
            "message": "Success",
            "data": {"data": latest_data}
        }
    except Exception as e:
        logger.error(f"Error getting latest data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/update")
async def update_data(params: DataUpdateParams):
    """æ›´æ–°æ•°æ®"""
    try:
        # TODO: æ ¹æ®paramså‚æ•°è§¦å‘æ•°æ®é‡æ–°åˆ†æ
        logger.info(f"Updating data with params: {params.model_dump()}")

        # è¿™é‡Œå¯ä»¥è§¦å‘æ•°æ®é‡æ–°åˆ†æ
        # æš‚æ—¶è¿”å›æˆåŠŸå“åº”
        return {
            "code": 200,
            "message": "æ•°æ®æ›´æ–°è¯·æ±‚å·²æäº¤",
            "data": {
                "success": True,
                "message": "æ•°æ®æ›´æ–°ä¸­ï¼Œè¯·ç¨åæŸ¥çœ‹ç»“æœ",
                "taskId": f"update_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            }
        }
    except Exception as e:
        logger.error(f"Error updating data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/update/progress/{task_id}")
async def get_update_progress(task_id: str):
    """è·å–æ•°æ®æ›´æ–°è¿›åº¦"""
    try:
        # TODO: æ ¹æ®task_idæŸ¥è¯¢å®é™…è¿›åº¦
        logger.info(f"Getting update progress for task: {task_id}")

        # æ¨¡æ‹Ÿè¿›åº¦æŸ¥è¯¢
        return {
            "code": 200,
            "message": "Success",
            "data": {
                "progress": 100,
                "status": "completed",
                "message": "æ•°æ®æ›´æ–°å®Œæˆ"
            }
        }
    except Exception as e:
        logger.error(f"Error getting update progress: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/export")
async def export_data(params: DataExportParams):
    """å¯¼å‡ºæ•°æ®"""
    try:
        # TODO: æ ¹æ®paramså‚æ•°å®ç°æ•°æ®å¯¼å‡ºé€»è¾‘
        logger.info(f"Exporting data with params: {params.model_dump()}")

        # è¿™é‡Œå®ç°æ•°æ®å¯¼å‡ºé€»è¾‘
        # æš‚æ—¶è¿”å›æˆåŠŸå“åº”
        return {
            "code": 200,
            "message": "æ•°æ®å¯¼å‡ºè¯·æ±‚å·²æäº¤",
            "data": {
                "taskId": f"export_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "status": "processing"
            }
        }
    except Exception as e:
        logger.error(f"Error exporting data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/export/status/{task_id}")
async def get_export_status(task_id: str):
    """è·å–å¯¼å‡ºä»»åŠ¡çŠ¶æ€"""
    try:
        return {
            "code": 200,
            "message": "Success",
            "data": {
                "status": "completed",
                "progress": 100,
                "downloadUrl": f"/api/downloads/{task_id}.csv"
            }
        }
    except Exception as e:
        logger.error(f"Error getting export status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/realtime")
async def get_realtime_updates():
    """è·å–å®æ—¶æ•°æ®æ›´æ–°"""
    try:
        return {
            "code": 200,
            "message": "Success",
            "data": {
                "lastUpdate": datetime.now().isoformat(),
                "hasNewData": False,
                "newDataCount": 0,
                "metrics": [
                    {"title": "æ–‡çŒ®æ€»é‡", "value": "52,489", "trend": "+12.5%"},
                    {"title": "å›¾æ–‡æ•°æ®é›†", "value": "15,932", "trend": "+8.3%"},
                    {"title": "æ•°æ®è¦†ç›–ç‡", "value": "25.8%", "trend": "+2.1%"},
                    {"title": "æ•°æ®æºç±»å‹", "value": "8", "trend": "+1"}
                ]
            }
        }
    except Exception as e:
        logger.error(f"Error getting realtime updates: {e}")
        raise HTTPException(status_code=500, detail=str(e))
