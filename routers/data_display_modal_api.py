"""
æ•°æ®å±•ç¤ºæ¨¡æ€æ¡†åç«¯æ¥å£
ä¸º DataDisplayModal ç»„ä»¶æä¾›å®Œæ•´çš„APIæ”¯æŒ
ä½¿ç”¨ get_auto_analysis_result ä½œä¸ºä¸»è¦æ•°æ®æº
"""

from fastapi import APIRouter, HTTPException, Query, BackgroundTasks
from pydantic import BaseModel, Field
from typing import Dict, List, Optional, Any, Union
import logging
import asyncio
import uuid
import time
import random
from datetime import datetime, timedelta
from enum import Enum

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

router = APIRouter(prefix="/data-sources", tags=["æ•°æ®å±•ç¤ºæ¨¡æ€æ¡†"])

# ==================== æ•°æ®æ¨¡å‹å®šä¹‰ ====================

class DataSourceOption(BaseModel):
    """æ•°æ®æºé€‰é¡¹"""
    label: str
    value: str

class MetricData(BaseModel):
    """æŒ‡æ ‡æ•°æ®"""
    title: str
    value: str
    trend: str

class LatestDataItem(BaseModel):
    """æœ€æ–°æ•°æ®é¡¹"""
    title: str
    type: str
    dataSize: int
    updateDate: str

class ChartData(BaseModel):
    """å›¾è¡¨æ•°æ®"""
    categories: List[str]
    values: List[int]

class DataSourceConfigResponse(BaseModel):
    """æ•°æ®æºé…ç½®å“åº”"""
    dataSourceOptions: List[DataSourceOption]
    fieldOptions: List[str]
    subTypeOptions: Dict[str, List[str]]
    exportOptions: List[str]

class DataStatsResponse(BaseModel):
    """æ•°æ®ç»Ÿè®¡å“åº”"""
    metricData: List[MetricData]
    chartData: ChartData
    latestData: List[LatestDataItem]

class DataUpdateParams(BaseModel):
    """æ•°æ®æ›´æ–°å‚æ•°"""
    dataSource: str
    selectedSubTypes: List[str]
    dateRange: List[str]  # [start_date, end_date]
    selectedFields: List[str]

class DataExportParams(BaseModel):
    """æ•°æ®å¯¼å‡ºå‚æ•°"""
    format: str
    dataSource: Optional[str] = None
    dateRange: Optional[List[str]] = None
    fields: Optional[List[str]] = None

class TaskStatus(str, Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

class ExportStatus(str, Enum):
    """å¯¼å‡ºçŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

# ==================== é…ç½®æ•°æ® ====================

# æ•°æ®æºé€‰é¡¹é…ç½®
DATA_SOURCE_OPTIONS = [
    DataSourceOption(label="ğŸ“š å­¦æœ¯è®ºæ–‡", value="academic_papers"),
    DataSourceOption(label="ğŸ“Š è°ƒæŸ¥æŠ¥å‘Š", value="survey_reports"),
    DataSourceOption(label="ğŸ“– ä¸“ä¸šä¹¦ç±", value="professional_books"),
    DataSourceOption(label="ğŸ“œ æ”¿ç­–æ–‡ä»¶", value="policy_documents"),
    DataSourceOption(label="âš–ï¸ æ³•è§„æ ‡å‡†", value="regulations"),
]

# ç ”ç©¶é¢†åŸŸé€‰é¡¹
FIELD_OPTIONS = [
    "è®¡ç®—æœºç§‘å­¦", "äººå·¥æ™ºèƒ½", "æ•°æ®ç§‘å­¦", "æœºå™¨å­¦ä¹ ",
    "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†", "è®¡ç®—æœºè§†è§‰", "çŸ¥è¯†å›¾è°±"
]

# å­ç±»å‹é€‰é¡¹æ˜ å°„
SUB_TYPE_OPTIONS = {
    "academic_papers": ["æœŸåˆŠè®ºæ–‡", "ä¼šè®®è®ºæ–‡", "å­¦ä½è®ºæ–‡", "é¢„å°æœ¬"],
    "survey_reports": ["è¡Œä¸šæŠ¥å‘Š", "å¸‚åœºè°ƒç ”", "ç”¨æˆ·ç ”ç©¶", "æŠ€æœ¯è¯„ä¼°"],
    "professional_books": ["æ•™æ", "ä¸“è‘—", "å‚è€ƒä¹¦", "æŠ€æœ¯æ‰‹å†Œ"],
    "policy_documents": ["å›½å®¶æ”¿ç­–", "è¡Œä¸šæ”¿ç­–", "åœ°æ–¹æ”¿ç­–", "å›½é™…æ”¿ç­–"],
    "regulations": ["å›½å®¶æ ‡å‡†", "è¡Œä¸šæ ‡å‡†", "ä¼ä¸šæ ‡å‡†", "å›½é™…æ ‡å‡†"]
}

# å¯¼å‡ºæ ¼å¼é€‰é¡¹
EXPORT_OPTIONS = ["CSV", "Excel", "PDF", "JSON"]

# å…¨å±€ä»»åŠ¡å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨æ•°æ®åº“ï¼‰
update_tasks: Dict[str, Dict] = {}
export_tasks: Dict[str, Dict] = {}

# ==================== è¾…åŠ©å‡½æ•° ====================

def map_resource_to_data_type(resource_name: str) -> str:
    """å°†èµ„æºåç§°æ˜ å°„åˆ°æ•°æ®ç±»å‹"""
    name_lower = resource_name.lower()
    
    if any(keyword in name_lower for keyword in ["è®ºæ–‡", "paper", "å­¦æœ¯", "ç ”ç©¶"]):
        return "å­¦æœ¯è®ºæ–‡"
    elif any(keyword in name_lower for keyword in ["æŠ¥å‘Š", "report", "è°ƒç ”", "åˆ†æ"]):
        return "è°ƒæŸ¥æŠ¥å‘Š"
    elif any(keyword in name_lower for keyword in ["ä¹¦ç±", "book", "æ•™æ", "ä¸“è‘—"]):
        return "ä¸“ä¸šä¹¦ç±"
    elif any(keyword in name_lower for keyword in ["æ”¿ç­–", "policy", "è§„å®š", "åˆ¶åº¦"]):
        return "æ”¿ç­–æ–‡ä»¶"
    elif any(keyword in name_lower for keyword in ["æ³•è§„", "æ ‡å‡†", "è§„èŒƒ", "æ¡ä¾‹"]):
        return "æ³•è§„æ ‡å‡†"
    else:
        return "å­¦æœ¯è®ºæ–‡"  # é»˜è®¤ç±»å‹

async def get_auto_analysis_data():
    """è·å–è‡ªåŠ¨åˆ†ææ•°æ®"""
    try:
        from services.resource_service import ResourceService
        return await ResourceService.get_auto_analysis_result()
    except Exception as e:
        logger.error(f"Failed to get auto analysis result: {e}")
        return None

def generate_task_id() -> str:
    """ç”Ÿæˆä»»åŠ¡ID"""
    return f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"

async def simulate_task_progress(task_id: str, task_dict: Dict[str, Dict], duration: int = 5):
    """æ¨¡æ‹Ÿä»»åŠ¡è¿›åº¦"""
    if task_id not in task_dict:
        return
    
    task = task_dict[task_id]
    start_time = time.time()
    
    # æ¨¡æ‹Ÿè¿›åº¦æ›´æ–°
    for progress in range(0, 101, 10):
        if task_id not in task_dict:  # ä»»åŠ¡å¯èƒ½è¢«å–æ¶ˆ
            return
            
        task["progress"] = progress
        task["status"] = TaskStatus.RUNNING if progress < 100 else TaskStatus.COMPLETED
        
        if progress < 100:
            await asyncio.sleep(duration / 10)
    
    # å®Œæˆä»»åŠ¡
    if task_id in task_dict:
        task["status"] = TaskStatus.COMPLETED
        task["progress"] = 100
        task["endTime"] = datetime.now().isoformat()
        task["duration"] = int((time.time() - start_time) * 1000)

# ==================== APIæ¥å£ ====================

@router.get("/config")
async def get_data_source_config():
    """è·å–æ•°æ®æºé…ç½®ä¿¡æ¯"""
    try:
        config = DataSourceConfigResponse(
            dataSourceOptions=DATA_SOURCE_OPTIONS,
            fieldOptions=FIELD_OPTIONS,
            subTypeOptions=SUB_TYPE_OPTIONS,
            exportOptions=EXPORT_OPTIONS
        )
        
        return {
            "code": 200,
            "message": "Success",
            "data": config.model_dump()
        }
    except Exception as e:
        logger.error(f"Error getting data source config: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{data_source}/sub-types")
async def get_sub_type_options(data_source: str):
    """æ ¹æ®æ•°æ®æºç±»å‹è·å–å­ç±»å‹é€‰é¡¹"""
    try:
        sub_types = SUB_TYPE_OPTIONS.get(data_source, [])

        return {
            "code": 200,
            "message": "Success",
            "data": {"subTypes": sub_types}
        }
    except Exception as e:
        logger.error(f"Error getting sub type options: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/stats")
async def get_data_stats(
    dataSource: Optional[str] = Query(None, description="æ•°æ®æºç±»å‹"),
    dateRange: Optional[str] = Query(None, description="æ—¥æœŸèŒƒå›´"),
    fields: Optional[str] = Query(None, description="ç ”ç©¶é¢†åŸŸ")
):
    """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨get_auto_analysis_resultä½œä¸ºæ•°æ®æº"""
    try:
        logger.info(f"Getting data stats for dataSource: {dataSource}, dateRange: {dateRange}, fields: {fields}")

        # è·å–è‡ªåŠ¨åˆ†æç»“æœ
        auto_analysis_data = await get_auto_analysis_data()

        if auto_analysis_data and len(auto_analysis_data) > 0:
            # åŸºäºçœŸå®æ•°æ®ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            # auto_analysis_data æ˜¯å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å« {"id", "name", "count", "icon", "color", "files"}
            total_count = sum(item.get("count", 0) for item in auto_analysis_data)

            # ç”ŸæˆæŒ‡æ ‡æ•°æ®
            metric_data = [
                MetricData(title="æ–‡çŒ®æ€»é‡", value=f"{total_count:,}", trend="+12.5%"),
                MetricData(title="å›¾æ–‡æ•°æ®é›†", value=f"{int(total_count * 0.3):,}", trend="+8.3%"),
                MetricData(title="æ•°æ®è¦†ç›–ç‡", value="25.8%", trend="+2.1%"),
                MetricData(title="æ•°æ®æºç±»å‹", value=str(len(auto_analysis_data)), trend="+1")
            ]

            # ç”Ÿæˆå›¾è¡¨æ•°æ®ï¼ˆå–å‰5ä¸ªèµ„æºï¼‰
            categories = [item.get("name", "æœªçŸ¥") for item in auto_analysis_data[:5]]
            values = [item.get("count", 0) for item in auto_analysis_data[:5]]
            chart_data = ChartData(categories=categories, values=values)

            # ç”Ÿæˆæœ€æ–°æ•°æ®åˆ—è¡¨
            latest_data = []
            for i, item in enumerate(auto_analysis_data[:5]):
                item_name = item.get("name", "æœªçŸ¥")
                item_count = item.get("count", 0)
                data_type = map_resource_to_data_type(item_name)
                latest_data.append(LatestDataItem(
                    title=item_name,
                    type=data_type,
                    dataSize=item_count,
                    updateDate=(datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
                ))
        else:
            # ä½¿ç”¨é»˜è®¤æ•°æ®
            logger.info("Using default data as auto analysis result is not available")
            metric_data = [
                MetricData(title="æ–‡çŒ®æ€»é‡", value="52,489", trend="+12.5%"),
                MetricData(title="å›¾æ–‡æ•°æ®é›†", value="15,932", trend="+8.3%"),
                MetricData(title="æ•°æ®è¦†ç›–ç‡", value="25.8%", trend="+2.1%"),
                MetricData(title="æ•°æ®æºç±»å‹", value="8", trend="+1")
            ]

            chart_data = ChartData(
                categories=["å­¦æœ¯è®ºæ–‡", "è°ƒæŸ¥æŠ¥å‘Š", "ä¸“ä¸šä¹¦ç±", "æ”¿ç­–æ–‡ä»¶", "æ³•è§„æ ‡å‡†"],
                values=[25000, 12000, 8000, 4500, 3000]
            )

            latest_data = [
                LatestDataItem(title="äººå·¥æ™ºèƒ½å‘å±•ç™½çš®ä¹¦", type="è°ƒæŸ¥æŠ¥å‘Š", dataSize=1250, updateDate="2024-01-15"),
                LatestDataItem(title="æ•°æ®å®‰å…¨æ ‡å‡†è§„èŒƒ", type="æ³•è§„æ ‡å‡†", dataSize=856, updateDate="2024-01-14"),
                LatestDataItem(title="æœºå™¨å­¦ä¹ æœ€æ–°è¿›å±•", type="å­¦æœ¯è®ºæ–‡", dataSize=2103, updateDate="2024-01-13"),
                LatestDataItem(title="çŸ¥è¯†å›¾è°±åº”ç”¨ç ”ç©¶", type="å­¦æœ¯è®ºæ–‡", dataSize=1587, updateDate="2024-01-12"),
                LatestDataItem(title="è¡Œä¸šæ•°å­—åŒ–è½¬å‹æŠ¥å‘Š", type="è°ƒæŸ¥æŠ¥å‘Š", dataSize=945, updateDate="2024-01-10")
            ]

        stats_response = DataStatsResponse(
            metricData=metric_data,
            chartData=chart_data,
            latestData=latest_data
        )

        return {
            "code": 200,
            "message": "Success",
            "data": stats_response.model_dump()
        }
    except Exception as e:
        logger.error(f"Error getting data stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/metrics")
async def get_metric_data():
    """è·å–æŒ‡æ ‡å¡æ•°æ®"""
    try:
        auto_analysis_data = await get_auto_analysis_data()

        if auto_analysis_data and len(auto_analysis_data) > 0:
            total_count = sum(item.get("count", 0) for item in auto_analysis_data)
            metrics = [
                MetricData(title="æ–‡çŒ®æ€»é‡", value=f"{total_count:,}", trend="+12.5%"),
                MetricData(title="å›¾æ–‡æ•°æ®é›†", value=f"{int(total_count * 0.3):,}", trend="+8.3%"),
                MetricData(title="æ•°æ®è¦†ç›–ç‡", value="25.8%", trend="+2.1%"),
                MetricData(title="æ•°æ®æºç±»å‹", value=str(len(auto_analysis_data)), trend="+1")
            ]
        else:
            metrics = [
                MetricData(title="æ–‡çŒ®æ€»é‡", value="52,489", trend="+12.5%"),
                MetricData(title="å›¾æ–‡æ•°æ®é›†", value="15,932", trend="+8.3%"),
                MetricData(title="æ•°æ®è¦†ç›–ç‡", value="25.8%", trend="+2.1%"),
                MetricData(title="æ•°æ®æºç±»å‹", value="8", trend="+1")
            ]

        return {
            "code": 200,
            "message": "Success",
            "data": {"metrics": [metric.model_dump() for metric in metrics]}
        }
    except Exception as e:
        logger.error(f"Error getting metric data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/chart-data")
async def get_chart_data(
    dataSource: Optional[str] = Query(None, description="æ•°æ®æºç±»å‹"),
    dateRange: Optional[str] = Query(None, description="æ—¥æœŸèŒƒå›´")
):
    """è·å–å›¾è¡¨æ•°æ®"""
    try:
        logger.info(f"Getting chart data for dataSource: {dataSource}, dateRange: {dateRange}")

        auto_analysis_data = await get_auto_analysis_data()

        if auto_analysis_data and len(auto_analysis_data) > 0:
            categories = [item.get("name", "æœªçŸ¥") for item in auto_analysis_data[:5]]
            values = [item.get("count", 0) for item in auto_analysis_data[:5]]
        else:
            categories = ["å­¦æœ¯è®ºæ–‡", "è°ƒæŸ¥æŠ¥å‘Š", "ä¸“ä¸šä¹¦ç±", "æ”¿ç­–æ–‡ä»¶", "æ³•è§„æ ‡å‡†"]
            values = [25000, 12000, 8000, 4500, 3000]

        chart_data = ChartData(categories=categories, values=values)

        return {
            "code": 200,
            "message": "Success",
            "data": chart_data.model_dump()
        }
    except Exception as e:
        logger.error(f"Error getting chart data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/latest")
async def get_latest_data(
    limit: Optional[int] = Query(5, description="è¿”å›æ•°é‡é™åˆ¶"),
    dataSource: Optional[str] = Query(None, description="æ•°æ®æºç±»å‹"),
    type: Optional[str] = Query(None, description="æ•°æ®ç±»å‹")
):
    """è·å–æœ€æ–°æ•°æ®åˆ—è¡¨"""
    try:
        logger.info(f"Getting latest data with limit: {limit}, dataSource: {dataSource}, type: {type}")

        auto_analysis_data = await get_auto_analysis_data()

        if auto_analysis_data and len(auto_analysis_data) > 0:
            latest_data = []
            for i, item in enumerate(auto_analysis_data[:limit]):
                item_name = item.get("name", "æœªçŸ¥")
                item_count = item.get("count", 0)
                data_type = map_resource_to_data_type(item_name)
                latest_data.append(LatestDataItem(
                    title=item_name,
                    type=data_type,
                    dataSize=item_count,
                    updateDate=(datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
                ))
        else:
            default_data = [
                LatestDataItem(title="äººå·¥æ™ºèƒ½å‘å±•ç™½çš®ä¹¦", type="è°ƒæŸ¥æŠ¥å‘Š", dataSize=1250, updateDate="2024-01-15"),
                LatestDataItem(title="æ•°æ®å®‰å…¨æ ‡å‡†è§„èŒƒ", type="æ³•è§„æ ‡å‡†", dataSize=856, updateDate="2024-01-14"),
                LatestDataItem(title="æœºå™¨å­¦ä¹ æœ€æ–°è¿›å±•", type="å­¦æœ¯è®ºæ–‡", dataSize=2103, updateDate="2024-01-13"),
                LatestDataItem(title="çŸ¥è¯†å›¾è°±åº”ç”¨ç ”ç©¶", type="å­¦æœ¯è®ºæ–‡", dataSize=1587, updateDate="2024-01-12"),
                LatestDataItem(title="è¡Œä¸šæ•°å­—åŒ–è½¬å‹æŠ¥å‘Š", type="è°ƒæŸ¥æŠ¥å‘Š", dataSize=945, updateDate="2024-01-10")
            ]
            latest_data = default_data[:limit]

        return {
            "code": 200,
            "message": "Success",
            "data": {"data": [item.model_dump() for item in latest_data]}
        }
    except Exception as e:
        logger.error(f"Error getting latest data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/update")
async def update_data(params: DataUpdateParams, background_tasks: BackgroundTasks):
    """æ›´æ–°æ•°æ®"""
    try:
        logger.info(f"Updating data with params: {params.model_dump()}")

        # ç”Ÿæˆä»»åŠ¡ID
        task_id = generate_task_id()

        # åˆ›å»ºä»»åŠ¡è®°å½•
        task = {
            "taskId": task_id,
            "status": TaskStatus.PENDING,
            "progress": 0,
            "startTime": datetime.now().isoformat(),
            "params": params.model_dump()
        }

        update_tasks[task_id] = task

        # å¯åŠ¨åå°ä»»åŠ¡æ¨¡æ‹Ÿæ•°æ®æ›´æ–°
        background_tasks.add_task(simulate_task_progress, task_id, update_tasks, 8)

        return {
            "code": 200,
            "message": "æ•°æ®æ›´æ–°è¯·æ±‚å·²æäº¤",
            "data": {
                "success": True,
                "message": "æ•°æ®æ›´æ–°ä¸­ï¼Œè¯·ç¨åæŸ¥çœ‹ç»“æœ",
                "taskId": task_id
            }
        }
    except Exception as e:
        logger.error(f"Error updating data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/update/progress/{task_id}")
async def get_update_progress(task_id: str):
    """è·å–æ•°æ®æ›´æ–°è¿›åº¦"""
    try:
        logger.info(f"Getting update progress for task: {task_id}")

        if task_id not in update_tasks:
            raise HTTPException(status_code=404, detail="Task not found")

        task = update_tasks[task_id]

        return {
            "code": 200,
            "message": "Success",
            "data": {
                "progress": task["progress"],
                "status": task["status"],
                "message": f"æ•°æ®æ›´æ–°è¿›åº¦: {task['progress']}%"
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting update progress: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/export")
async def export_data(params: DataExportParams, background_tasks: BackgroundTasks):
    """å¯¼å‡ºæ•°æ®"""
    try:
        logger.info(f"Exporting data with params: {params.model_dump()}")

        # ç”Ÿæˆä»»åŠ¡ID
        task_id = generate_task_id()

        # åˆ›å»ºå¯¼å‡ºä»»åŠ¡è®°å½•
        task = {
            "taskId": task_id,
            "status": ExportStatus.PENDING,
            "progress": 0,
            "startTime": datetime.now().isoformat(),
            "params": params.model_dump(),
            "format": params.format
        }

        export_tasks[task_id] = task

        # å¯åŠ¨åå°ä»»åŠ¡æ¨¡æ‹Ÿæ•°æ®å¯¼å‡º
        background_tasks.add_task(simulate_export_task, task_id, params.format)

        return {
            "code": 200,
            "message": "æ•°æ®å¯¼å‡ºè¯·æ±‚å·²æäº¤",
            "data": {
                "taskId": task_id,
                "status": ExportStatus.PROCESSING
            }
        }
    except Exception as e:
        logger.error(f"Error exporting data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/export/status/{task_id}")
async def get_export_status(task_id: str):
    """è·å–å¯¼å‡ºä»»åŠ¡çŠ¶æ€"""
    try:
        logger.info(f"Getting export status for task: {task_id}")

        if task_id not in export_tasks:
            raise HTTPException(status_code=404, detail="Export task not found")

        task = export_tasks[task_id]

        response_data = {
            "status": task["status"],
            "progress": task["progress"]
        }

        # å¦‚æœä»»åŠ¡å®Œæˆï¼Œæ·»åŠ ä¸‹è½½é“¾æ¥
        if task["status"] == ExportStatus.COMPLETED:
            response_data["downloadUrl"] = f"/api/downloads/{task_id}.{task['format'].lower()}"

        # å¦‚æœä»»åŠ¡å¤±è´¥ï¼Œæ·»åŠ é”™è¯¯ä¿¡æ¯
        if task["status"] == ExportStatus.FAILED:
            response_data["error"] = task.get("error", "å¯¼å‡ºå¤±è´¥")

        return {
            "code": 200,
            "message": "Success",
            "data": response_data
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting export status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/realtime")
async def get_realtime_updates():
    """è·å–å®æ—¶æ•°æ®æ›´æ–°"""
    try:
        # è·å–æœ€æ–°çš„æŒ‡æ ‡æ•°æ®
        auto_analysis_data = await get_auto_analysis_data()

        if auto_analysis_data and len(auto_analysis_data) > 0:
            total_count = sum(item.get("count", 0) for item in auto_analysis_data)
            metrics = [
                MetricData(title="æ–‡çŒ®æ€»é‡", value=f"{total_count:,}", trend="+12.5%"),
                MetricData(title="å›¾æ–‡æ•°æ®é›†", value=f"{int(total_count * 0.3):,}", trend="+8.3%"),
                MetricData(title="æ•°æ®è¦†ç›–ç‡", value="25.8%", trend="+2.1%"),
                MetricData(title="æ•°æ®æºç±»å‹", value=str(len(auto_analysis_data)), trend="+1")
            ]
        else:
            metrics = [
                MetricData(title="æ–‡çŒ®æ€»é‡", value="52,489", trend="+12.5%"),
                MetricData(title="å›¾æ–‡æ•°æ®é›†", value="15,932", trend="+8.3%"),
                MetricData(title="æ•°æ®è¦†ç›–ç‡", value="25.8%", trend="+2.1%"),
                MetricData(title="æ•°æ®æºç±»å‹", value="8", trend="+1")
            ]

        return {
            "code": 200,
            "message": "Success",
            "data": {
                "lastUpdate": datetime.now().isoformat(),
                "hasNewData": random.choice([True, False]),  # æ¨¡æ‹Ÿéšæœºæ›´æ–°
                "newDataCount": random.randint(0, 5),
                "metrics": [metric.model_dump() for metric in metrics]
            }
        }
    except Exception as e:
        logger.error(f"Error getting realtime updates: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/monitoring/status")
async def get_monitoring_status():
    """è·å–ç›®å½•ç›‘å¬çŠ¶æ€"""
    try:
        from services.directory_monitor_service import get_monitoring_status

        status = get_monitoring_status()

        return {
            "code": 200,
            "message": "Success",
            "data": status
        }
    except Exception as e:
        logger.error(f"Error getting monitoring status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== è¾…åŠ©ä»»åŠ¡å‡½æ•° ====================

async def simulate_export_task(task_id: str, format: str):
    """æ¨¡æ‹Ÿå¯¼å‡ºä»»åŠ¡"""
    if task_id not in export_tasks:
        return

    task = export_tasks[task_id]
    start_time = time.time()

    try:
        logger.info(f"Starting export task {task_id} with format: {format}")

        # æ¨¡æ‹Ÿå¯¼å‡ºè¿›åº¦
        for progress in range(0, 101, 20):
            if task_id not in export_tasks:  # ä»»åŠ¡å¯èƒ½è¢«å–æ¶ˆ
                return

            task["progress"] = progress
            task["status"] = ExportStatus.PROCESSING if progress < 100 else ExportStatus.COMPLETED

            if progress < 100:
                await asyncio.sleep(1)  # å¯¼å‡ºä»»åŠ¡ç¨å¿«ä¸€äº›

        # å®Œæˆä»»åŠ¡
        if task_id in export_tasks:
            task["status"] = ExportStatus.COMPLETED
            task["progress"] = 100
            task["endTime"] = datetime.now().isoformat()
            task["duration"] = int((time.time() - start_time) * 1000)
            logger.info(f"Export task {task_id} completed successfully")

    except Exception as e:
        logger.error(f"Export task {task_id} failed: {e}")
        if task_id in export_tasks:
            task["status"] = ExportStatus.FAILED
            task["error"] = str(e)
