import sys
import os
# ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨DeepSeek API
from openai import OpenAI
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.resource import ResourceItem
import pathlib
from typing import List, Dict, Tuple, Any
import random
import json
import hashlib
import aiohttp
import asyncio
import logging
from datetime import datetime, timedelta
import uuid
import time
from concurrent.futures import ThreadPoolExecutor
import re
from bson import ObjectId
import concurrent.futures
import multiprocessing
from services.database import DataSource, Task
from pymongo import UpdateOne
# å¯¼å…¥é…ç½®
from config import config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def collect_files_mp(base_dir):
    result = []
    try:
        for root, _, files in os.walk(base_dir):
            for file in files:
                # ".json"
                if file.lower().endswith((".pdf")):
                    result.append(os.path.join(root, file))
    except Exception as e:
        print(f"Error walking {base_dir}: {e}")
    print(f"Finish scanning: {base_dir}, found {len(result)} files")
    return result

def get_all_dirs(base_dirs, max_depth=1):
    # é€’å½’æ”¶é›†æ‰€æœ‰å­ç›®å½•ï¼Œé™åˆ¶æœ€å¤§æ·±åº¦
    all_dirs = []
    for base in base_dirs:
        for root, dirs, _ in os.walk(base):
            depth = root[len(base):].count(os.sep)
            if depth <= max_depth:
                all_dirs.append(root)
    return all_dirs

class ResourceService:
    """èµ„æºæœåŠ¡ç±» - ä½¿ç”¨MongoDBè¿›è¡Œä»»åŠ¡ç®¡ç†"""
    
    # ç¼“å­˜åˆ†ç±»ç»“æœï¼Œé¿å…é¢‘ç¹è¯·æ±‚å¤§æ¨¡å‹API
    _cache = {}
    _cache_time = None
    _cache_duration = timedelta(hours=1)  # ç¼“å­˜æœ‰æ•ˆæœŸ1å°æ—¶
    
    # æ·»åŠ ç±»å˜é‡ç”¨äºå­˜å‚¨è‡ªåŠ¨åˆ†æçš„ç»“æœ
    _auto_analysis_result = None
    _auto_analysis_time = None
    _auto_analysis_running = False

    # æ·»åŠ ä»»åŠ¡è·Ÿè¸ªå­—å…¸
    _analysis_tasks = {}

    @staticmethod
    async def get_resource_data() -> list[ResourceItem]:
        """è·å–èµ„æºæ•°æ®åˆ—è¡¨"""
        # ç›´æ¥è¿”å›è‡ªåŠ¨åˆ†æç»“æœ
        auto_analysis_result = await ResourceService.get_auto_analysis_result()
        if auto_analysis_result:
            return auto_analysis_result
        
        # å¦‚æœè‡ªåŠ¨åˆ†æç»“æœä¸ºç©ºï¼Œåˆ™ä½¿ç”¨åŸæœ‰é€»è¾‘
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        now = datetime.now()
        if (ResourceService._cache and ResourceService._cache_time and 
            now - ResourceService._cache_time < ResourceService._cache_duration):
            logger.info("Using cached resource data")
            return ResourceService._cache
        
        # æ‰«æç›®å½•è·¯å¾„ (å¯ä»¥ä»é…ç½®æ–‡ä»¶è¯»å–)
        base_dir = os.path.join(os.path.expanduser("~"), "Documents")
        
        # æ”¶é›†æ–‡ä»¶å¤¹ä¿¡æ¯
        folder_info = await ResourceService._collect_folder_info(base_dir)
        
        # æ™ºèƒ½åˆ†ææ–‡ä»¶å¤¹å¹¶ç”Ÿæˆåˆ†ç±»
        categories = await ResourceService._analyze_and_categorize_folders(folder_info)
        
        # ç»Ÿè®¡å„ç±»èµ„æºæ•°é‡
        result = []
        for i, (category, folders) in enumerate(categories.items(), 1):
            # ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆä¸€ä¸ªç¨³å®šçš„é¢œè‰²
            color = ResourceService._generate_color(category)
            # ä¸ºæ¯ä¸ªåˆ†ç±»é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„å›¾æ ‡
            icon = ResourceService._select_icon(category)
            
            result.append(
                ResourceItem(
                    id=i,
                    name=category,
                    count=len(folders),
                    icon=icon,
                    color=color
                )
            )
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•èµ„æºï¼Œè¿”å›é»˜è®¤æ•°æ®
        if not result:
            result = [
                ResourceItem(id=1, name="æ–‡æ¡£", count=0, icon="ğŸ“„", color="#1890ff"),
                ResourceItem(id=2, name="å›¾åƒ", count=0, icon="ğŸ–¼ï¸", color="#52c41a"),
                ResourceItem(id=3, name="éŸ³é¢‘", count=0, icon="ğŸµ", color="#722ed1"),
                ResourceItem(id=4, name="è§†é¢‘", count=0, icon="ğŸ¬", color="#faad14"),
                ResourceItem(id=5, name="æ•°æ®", count=0, icon="ğŸ“Š", color="#13c2c2"),
            ]
        
        # æ›´æ–°ç¼“å­˜
        ResourceService._cache = result
        ResourceService._cache_time = now
        
        return result
    
    @staticmethod
    async def _collect_folder_info(base_dir: str) -> List[Dict]:
        """æ”¶é›†ç›®å½•ä¸­çš„æ–‡ä»¶å¤¹ä¿¡æ¯"""
        folder_info = []
        
        # å¦‚æœæ˜¯Cç›˜è·¯å¾„ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
        if base_dir.startswith("C:"):
            logger.info(f"Skipping C: drive path: {base_dir}")
            return folder_info
            
        try:
            # éå†ç›®å½•ï¼Œåªæ”¶é›†æ–‡ä»¶å¤¹
            for root, dirs, _ in os.walk(base_dir):
                for dir_name in dirs:
                    # è·³è¿‡éšè—ç›®å½•ã€ç³»ç»Ÿç›®å½•å’ŒCç›˜è·¯å¾„
                    if dir_name.startswith('.') or dir_name.startswith('$') or root.startswith("C:"):
                        continue
                    
                    dir_path = os.path.join(root, dir_name)
                    try:
                        # è·å–æ–‡ä»¶å¤¹ä¿¡æ¯
                        dir_stat = os.stat(dir_path)
                        
                        # è®¡ç®—æ–‡ä»¶å¤¹æ·±åº¦
                        depth = len(os.path.relpath(dir_path, base_dir).split(os.sep))
                        
                        # æ”¶é›†æ–‡ä»¶å¤¹ä¿¡æ¯
                        folder_info.append({
                            'name': dir_name,
                            'path': dir_path,
                            'depth': depth,
                            'modified': datetime.fromtimestamp(dir_stat.st_mtime).isoformat(),
                            'relative_path': os.path.relpath(dir_path, base_dir)
                        })
                    except (PermissionError, FileNotFoundError) as e:
                        logger.warning(f"Error accessing folder {dir_path}: {e}")
        except Exception as e:
            logger.error(f"Error scanning directory {base_dir}: {e}")
        
        return folder_info
    
    @staticmethod
    async def _analyze_and_categorize_folders(folder_info: List[Dict]) -> Dict[str, List[Dict]]:
        """åˆ†ææ–‡ä»¶å¤¹å¹¶ç”Ÿæˆåˆ†ç±»"""
        # ç›´æ¥ä½¿ç”¨åŸºäºæ–‡ä»¶å¤¹åç§°çš„æ™ºèƒ½åˆ†ç±»
        return ResourceService._smart_categorize_folders(folder_info)
    
    @staticmethod
    def _smart_categorize_folders(folder_info: List[Dict]) -> Dict[str, List[Dict]]:
        """
        åªåˆ†æpdfå’Œjsonæ–‡ä»¶çš„æ–‡ä»¶åï¼ˆä¸åˆ†ææ–‡ä»¶å¤¹åï¼‰ï¼Œå°†æ–‡ä»¶å¤¹å½’å…¥äº”å¤§ç±»ã€‚
        å¦‚æœæ²¡æœ‰å‘½ä¸­ä»»ä½•ç±»åˆ«ï¼Œåˆ™è¯¥æ–‡ä»¶å¤¹è¢«è¿‡æ»¤æ‰ã€‚
        """
        categories = {"å­¦æœ¯è®ºæ–‡": [], "è°ƒæŸ¥æŠ¥å‘Š": [], "ä¸“ä¸šä¹¦ç±": [], "æ”¿ç­–æ–‡ä»¶": [], "æ³•è§„æ ‡å‡†": []}
        # å…³é”®è¯æ˜ å°„
        keyword_map = {
            "å­¦æœ¯è®ºæ–‡": ["paper", "è®ºæ–‡", "thesis", "article"],
            "è°ƒæŸ¥æŠ¥å‘Š": ["report", "è°ƒæŸ¥", "survey"],
            "ä¸“ä¸šä¹¦ç±": ["book", "ä¸“è‘—", "æ•™æ", "manual", "handbook"],
            "æ”¿ç­–æ–‡ä»¶": ["policy", "æ”¿ç­–", "guideline", "è§„åˆ’"],
            "æ³•è§„æ ‡å‡†": ["regulation", "æ ‡å‡†", "è§„èŒƒ", "law", "æ¡ä¾‹"]
        }

        for folder in folder_info:
            folder_path = folder['path']
            file_type_count = {k: 0 for k in categories}
            # éå†è¯¥æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰pdf/jsonæ–‡ä»¶
            for root, _, files in os.walk(folder_path):
                for file in files:
                    if not file.lower().endswith(('.pdf', '.json')):
                        continue
                    fname = file.lower()
                    for cat, keywords in keyword_map.items():
                        if any(kw in fname for kw in keywords):
                            file_type_count[cat] += 1
                            break  # å‘½ä¸­ä¸€ä¸ªç±»åˆ«å°±ä¸å†ç»§ç»­
            # è¯¥æ–‡ä»¶å¤¹çš„åˆ†ç±»ç”±æœ€å¤šçš„é‚£ä¸€ç±»å†³å®š
            total_hits = sum(file_type_count.values())
            if total_hits == 0:
                continue  # è¯¥æ–‡ä»¶å¤¹ä¸‹æ²¡æœ‰å‘½ä¸­ä»»ä½•ç±»åˆ«çš„pdf/jsonæ–‡ä»¶ï¼Œè·³è¿‡
            main_cat = max(file_type_count, key=lambda k: file_type_count[k])
            categories[main_cat].append(folder)
        return categories
    
    @staticmethod
    def _classify_by_name_pattern(folder_name: str) -> str:
        """æ ¹æ®æ–‡ä»¶å¤¹åç§°æ¨¡å¼è¿›è¡Œåˆ†ç±»"""
        # æ•°å­—å¼€å¤´çš„æ–‡ä»¶å¤¹ï¼ˆå¯èƒ½æ˜¯æ—¥æœŸæˆ–ç‰ˆæœ¬ï¼‰
        if re.match(r'^\d+', folder_name):
            return "ç‰ˆæœ¬æˆ–æ—¥æœŸæ–‡ä»¶å¤¹"
        
        # åŒ…å«æ—¥æœŸæ ¼å¼çš„æ–‡ä»¶å¤¹
        if re.search(r'\d{4}[-_]\d{2}[-_]\d{2}', folder_name):
            return "æ—¥æœŸæ–‡ä»¶å¤¹"
        
        # å…¨å¤§å†™çš„æ–‡ä»¶å¤¹ï¼ˆå¯èƒ½æ˜¯ç¼©å†™æˆ–ç³»ç»Ÿæ–‡ä»¶å¤¹ï¼‰
        if folder_name.isupper() and len(folder_name) <= 10:
            return "ç³»ç»Ÿæˆ–ç¼©å†™æ–‡ä»¶å¤¹"
        
        # åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ–‡ä»¶å¤¹
        if re.search(r'[._-]', folder_name):
            return "ç‰¹æ®Šæ ¼å¼æ–‡ä»¶å¤¹"
        
        # å¾ˆé•¿çš„æ–‡ä»¶å¤¹åï¼ˆå¯èƒ½æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„ï¼‰
        if len(folder_name) > 30:
            return "è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å¤¹"
        
        return None
    
    @staticmethod
    def _generate_color(category: str) -> str:
        """ä¸ºåˆ†ç±»ç”Ÿæˆä¸€ä¸ªç¨³å®šçš„é¢œè‰²"""
        # ä½¿ç”¨åˆ†ç±»åç§°çš„å“ˆå¸Œå€¼ç”Ÿæˆé¢œè‰²ï¼Œç¡®ä¿åŒä¸€åˆ†ç±»æ€»æ˜¯å¾—åˆ°ç›¸åŒçš„é¢œè‰²
        hash_obj = hashlib.md5(category.encode())
        hash_int = int(hash_obj.hexdigest(), 16)
        
        # ç”ŸæˆæŸ”å’Œçš„é¢œè‰²
        r = (hash_int & 0xFF0000) >> 16
        g = (hash_int & 0x00FF00) >> 8
        b = hash_int & 0x0000FF
        
        # ç¡®ä¿é¢œè‰²ä¸ä¼šå¤ªæš—
        r = max(r, 100)
        g = max(g, 100)
        b = max(b, 100)
        
        return f"#{r:02x}{g:02x}{b:02x}"
    
    @staticmethod
    def _select_icon(category: str) -> str:
        """ä¸ºåˆ†ç±»é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„å›¾æ ‡"""
        # å®šä¹‰åˆ†ç±»å’Œå›¾æ ‡çš„æ˜ å°„
        category_icons = {
            "å·¥ä½œæ–‡æ¡£": "ğŸ’¼",
            "å­¦ä¹ èµ„æ–™": "ğŸ“š",
            "ä¸ªäººæ–‡ä»¶": "ğŸ‘¤",
            "å¼€å‘é¡¹ç›®": "ğŸ’»",
            "åª’ä½“æ–‡ä»¶": "ğŸ¬",
            "ä¸‹è½½æ–‡ä»¶": "ğŸ“¥",
            "ç³»ç»Ÿæ–‡ä»¶": "âš™ï¸",
            "å¨±ä¹å†…å®¹": "ğŸ®",
            "è´¢åŠ¡èµ„æ–™": "ğŸ’°",
            "å¥åº·åŒ»ç–—": "ğŸ¥",
            "ç‰ˆæœ¬æˆ–æ—¥æœŸæ–‡ä»¶å¤¹": "ğŸ“…",
            "æ—¥æœŸæ–‡ä»¶å¤¹": "ğŸ“…",
            "ç³»ç»Ÿæˆ–ç¼©å†™æ–‡ä»¶å¤¹": "ğŸ”¤",
            "ç‰¹æ®Šæ ¼å¼æ–‡ä»¶å¤¹": "ğŸ“", 
            "è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å¤¹": "ğŸ¤–",
            "å…¶ä»–æ–‡ä»¶å¤¹": "ğŸ“"
        }
        
        # å°è¯•ç²¾ç¡®åŒ¹é…
        if category in category_icons:
            return category_icons[category]
        
        # å°è¯•éƒ¨åˆ†åŒ¹é…
        for key, icon in category_icons.items():
            if key in category or category in key:
                return icon
        
        # é»˜è®¤å›¾æ ‡
        return "ğŸ“"

    @staticmethod
    async def start_analysis_task(base_dir: str, file_list=None, options=None) -> str:
        """å¯åŠ¨å¼‚æ­¥åˆ†æä»»åŠ¡ï¼Œå¹¶å°†ä»»åŠ¡çŠ¶æ€å­˜å…¥æ•°æ®åº“"""
        # å¦‚æœç›®å½•ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ç›®å½•
        if not base_dir or not os.path.exists(base_dir):
            base_dir = os.path.join(os.path.expanduser("~"), "Documents")
        
        # åˆ›å»ºä»»åŠ¡å¹¶å­˜å…¥æ•°æ®åº“
        task = Task(
            task_type="resource_analysis",
            related_id=base_dir # ä½¿ç”¨åŸºç¡€ç›®å½•ä½œä¸ºå…³è”ID
        )
        await task.insert()
        
        # å¯åŠ¨å¼‚æ­¥ä»»åŠ¡, task.id æ˜¯ ObjectId, éœ€è¦è½¬ä¸º str
        asyncio.create_task(ResourceService._run_analysis_task(str(task.id), base_dir, file_list, options))
        
        return str(task.id)

    @staticmethod
    async def get_task_status(task_id: str) -> Dict:
        """ä»æ•°æ®åº“è·å–ä»»åŠ¡çŠ¶æ€"""
        try:
            # Beanie's get method uses Pydantic's ObjectId, so we must convert the string
            task = await Task.get(ObjectId(task_id))
        except Exception:
            # This can happen if the task_id is not a valid ObjectId format
            logger.warning(f"Task not found or invalid ID format: {task_id}")
            return {"status": "not_found", "error": "Invalid ID format"}
        
        if not task:
            logger.warning(f"Task not found: {task_id}")
            return {"status": "not_found"}
        
        # ä½¿ç”¨ model_dump è¿”å› Pydantic æ¨¡å‹å…¼å®¹çš„å­—å…¸
        return task.model_dump(by_alias=True, exclude={'id'})

    @staticmethod
    async def _run_analysis_task(task_id: str, base_dir: str, file_list=None, options=None):
        """è¿è¡Œåˆ†æä»»åŠ¡ï¼Œå¹¶æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€"""
        task_obj_id = ObjectId(task_id)
        try:
            await Task.find_one(Task.id == task_obj_id).update({"$set": {"status": "running", "progress": 5}})
            
            if file_list:
                logger.info(f"Using provided file list with {len(file_list)} files")
                
                folder_structure = {}
                for file in file_list:
                    path = file["path"]
                    folder_path = os.path.dirname(path)
                    
                    if not folder_path: folder_path = "æ ¹ç›®å½•"
                    if folder_path not in folder_structure: folder_structure[folder_path] = []
                    folder_structure[folder_path].append(file)
                
                categories = {}
                for folder_path, files in folder_structure.items():
                    folder_name = os.path.basename(folder_path)
                    if not folder_name: folder_name = os.path.basename(os.path.dirname(folder_path))
                    if not folder_name: folder_name = "å…¶ä»–"
                    if folder_name not in categories: categories[folder_name] = []
                    categories[folder_name].extend(files)
                
                task_progress = 90
            else:
                folder_info = await ResourceService._collect_folder_info(base_dir)
                await Task.find_one(Task.id == task_obj_id).update({"$set": {"progress": 50}})
                
                categories = ResourceService._smart_categorize_folders(folder_info)
                task_progress = 90
            
            await Task.find_one(Task.id == task_obj_id).update({"$set": {"progress": task_progress}})

            result = []
            for category, items in categories.items():
                result.append({
                    "id": len(result) + 1, "name": category, "count": len(items),
                    "icon": ResourceService._select_icon(category), "color": ResourceService._generate_color(category),
                    "folders": items if not file_list else []
                })
            
            await Task.find_one(Task.id == task_obj_id).update({"$set": {
                "status": "completed", "progress": 100, 
                "result": {"categories": result}, "end_time": datetime.now()
            }})
            
            ResourceService._cache = result
            ResourceService._cache_time = datetime.now()
            
        except Exception as e:
            logger.error(f"Analysis task failed: {e}", exc_info=True)
            await Task.find_one(Task.id == task_obj_id).update({"$set": {
                "status": "failed", "error": str(e), "end_time": datetime.now()
            }})

    @staticmethod
    async def auto_analyze_local_directories(base_dir=None):
        """é€’å½’éå†æŒ‡å®šç›®å½•ï¼Œåªæ”¶é›† pdf å’Œ json æ–‡ä»¶ï¼ŒLLM åˆ†ç±»ï¼Œç»“æœå…¥åº“ï¼ˆåˆ†å—é€’å½’+å¤šè¿›ç¨‹ä¼˜åŒ–+åŠ¨æ€è¿›åº¦æ—¥å¿—ï¼‰"""
        if ResourceService._auto_analysis_running:
            logger.info("Auto analysis already running, skipping")
            return
        try:
            ResourceService._auto_analysis_running = True
            logger.info("Starting automatic analysis of local directories (recursive, pdf/json only, multiprocess, fine-grained)")

            # åˆ›å»ºä»»åŠ¡å¯¹è±¡å¹¶æ·»åŠ åˆ°ä»»åŠ¡è·Ÿè¸ªå­—å…¸
            task_id = str(uuid.uuid4())
            task_obj = type('AnalysisTask', (), {
                'id': task_id,
                'is_auto_analysis': True,
                'progress': 0,
                'status': 'running',
                'start_time': datetime.now()
            })()
            ResourceService._analysis_tasks[task_id] = task_obj

            import multiprocessing
            # æ–°å¢ï¼šå¦‚æœä¼ å…¥ base_dirï¼Œåˆ™åªæ‰«æè¯¥ç›®å½•ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤ç›®å½•
            if base_dir and os.path.exists(base_dir):
                scan_dirs = [base_dir]
            else:
                # ä½¿ç”¨é…ç½®ä¸­çš„åŸºç¡€ç›®å½•åˆ—è¡¨
                config_dirs = config.BASE_PDF_DIRS
                # è¿‡æ»¤å‡ºå­˜åœ¨çš„ç›®å½•
                scan_dirs = [d for d in config_dirs if os.path.exists(d)]

                # å¦‚æœé…ç½®çš„ç›®å½•éƒ½ä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨åŸæœ‰é€»è¾‘
                if not scan_dirs:
                    home_dir = os.path.expanduser("~")
                    drive_dirs = [f"{d}:\\" for d in "DEFGHIJKLMNOPQRSTUVWXYZ" if os.path.exists(f"{d}:\\")]
                    scan_dirs = drive_dirs if drive_dirs else [home_dir]
            common_dirs = [d for d in scan_dirs if not d.startswith("C:")]
            # é€’å½’æ”¶é›†æ‰€æœ‰å­ç›®å½•ï¼ˆå¦‚åˆ°2çº§ï¼‰
            all_start_dirs = get_all_dirs(common_dirs, max_depth=2)
            logger.info(f"Total start dirs to scan: {len(all_start_dirs)}")
            # ç”¨å¤šè¿›ç¨‹æ± åŠ¨æ€æ”¶é›†ï¼Œä¸»è¿›ç¨‹æŒç»­è¾“å‡ºè¿›åº¦
            all_files = []
            if all_start_dirs:
                with multiprocessing.get_context("spawn").Pool(processes=min(16, os.cpu_count() or 1)) as pool:
                    total = len(all_start_dirs)
                    for idx, files in enumerate(pool.imap_unordered(collect_files_mp, all_start_dirs), 1):
                        all_files.extend(files)
                        if idx % 10 == 0 or idx == total:
                            logger.info(f"å·²å®Œæˆ {idx}/{total} ä¸ªç›®å½•ï¼Œç´¯è®¡æ”¶é›†æ–‡ä»¶æ•°: {len(all_files)}")
            logger.info(f"Total pdf/json files collectedæ–‡ä»¶æ•°é‡: {len(all_files)}")

            # æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼šæ–‡ä»¶æ”¶é›†å®Œæˆ
            if 'task_obj' in locals():
                task_obj.progress = 30
                task_obj.status = 'analyzing'

            # 2. LLM åˆ†ç±»ï¼ˆå¤±è´¥åˆ™æœ¬åœ°è§„åˆ™ï¼‰
            try:
                file_dicts = [{'name': os.path.basename(f), 'path': f} for f in all_files]
                categories = await ResourceService._analyze_with_deepseek(file_dicts)

                # æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼šåˆ†æå®Œæˆ
                if 'task_obj' in locals():
                    task_obj.progress = 70
            except Exception as e:
                logger.warning(f"DeepSeek analysis failed: {e}, falling back to basic categorization")
                from services.alert_service import AlertService
                await AlertService.add_alert(
                    message=f"DeepSeek LLM åˆ†ç±»å¤±è´¥: {str(e)}ï¼Œå·²åˆ‡æ¢ä¸ºæœ¬åœ°è§„åˆ™",
                    level="warning",
                    extra={"task_type": "auto_resource_analysis"}
                )
                categories = ResourceService._smart_categorize_folders(
                    [{'name': os.path.basename(f), 'path': f} for f in all_files]
                )

            # 3. æ•´ç†åˆ†ç±»ç»“æœ
            result = [
                {"id": i + 1, "name": cat, "count": len(files),
                 "icon": ResourceService._select_icon(cat), "color": ResourceService._generate_color(cat),
                 "files": files[:50]}
                for i, (cat, files) in enumerate(categories.items())
            ]
            # 4. å†™å…¥æ•°æ®åº“
            from services.alert_service import AlertService
            existing_task = await Task.find_one(Task.task_type == "auto_resource_analysis")
            if existing_task:
                await Task.find_one(Task.id == existing_task.id).update({
                    "$set": {
                        "status": "completed",
                        "progress": 100,
                        "result": {"categories": result},
                        "end_time": datetime.now()
                    }
                })
            else:
                new_task = Task(task_type="auto_resource_analysis", status="completed", start_time=datetime.now(), end_time=datetime.now(), result={"categories": result})
                await new_task.insert()
            logger.info("Auto analysis completed and categories saved to DB.")

            # æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼šæ•°æ®ä¿å­˜å®Œæˆ
            if 'task_obj' in locals():
                task_obj.progress = 90

            # åŠ¨æ€å¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
            try:
                from services.auto_paper_import_service import AutoPaperImportService
                imported_count = await AutoPaperImportService.import_valid_papers_from_auto_analysis()
                logger.info(f"è‡ªåŠ¨åˆ†æåå·²å¯¼å…¥ {imported_count} ç¯‡æœ‰æ•ˆè®ºæ–‡ã€‚")

                # æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼šå…¨éƒ¨å®Œæˆ
                if 'task_obj' in locals():
                    task_obj.progress = 100
                    task_obj.status = 'completed'

            except Exception as e:
                logger.error(f"è‡ªåŠ¨å¯¼å…¥æœ‰æ•ˆè®ºæ–‡å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"Error in automatic analysis: {e}")
            from services.alert_service import AlertService
            await AlertService.add_alert(
                message=f"è‡ªåŠ¨åˆ†æä»»åŠ¡å¼‚å¸¸: {str(e)}",
                level="error",
                extra={"task_type": "auto_resource_analysis"}
            )
        finally:
            ResourceService._auto_analysis_running = False
            # æ¸…ç†ä»»åŠ¡è·Ÿè¸ª
            if 'task_id' in locals():
                if task_id in ResourceService._analysis_tasks:
                    ResourceService._analysis_tasks[task_id].status = 'completed'
                    # å¯ä»¥é€‰æ‹©ä¿ç•™ä»»åŠ¡ä¸€æ®µæ—¶é—´æˆ–ç«‹å³åˆ é™¤
                    # del ResourceService._analysis_tasks[task_id]
            logger.info("Auto analysis completed, reset running flag.")

    @staticmethod
    async def get_auto_analysis_result():
        """è·å–è‡ªåŠ¨åˆ†æç»“æœï¼ˆæ¯æ¬¡éƒ½æŸ¥æ•°æ®åº“ï¼Œä¸ç”¨å†…å­˜ç¼“å­˜ï¼‰"""
        try:
            # ç›´æ¥ä»æ•°æ®åº“ä»»åŠ¡ä¸­è·å–æœ€æ–°æˆåŠŸçš„ç»“æœ
            task = await Task.find_one(
                Task.task_type == "auto_resource_analysis",
                Task.status == "completed",
                sort=[("end_time", -1)]
            )
            if task and task.end_time and (datetime.now() - task.end_time < timedelta(hours=24)):
                logger.info("Loaded auto analysis result from completed DB task")
                result = task.result.get("categories") if task.result else None
                return result
        except Exception as e:
            logger.error(f"Failed to load analysis result from DB task: {e}")
        
        logger.info("No valid recent task found, starting new auto analysis")
        await ResourceService.auto_analyze_local_directories()
        # æ–°ä»»åŠ¡åˆšå¯åŠ¨æ—¶è¿˜æ²¡æœ‰ç»“æœï¼Œè¿™é‡Œè¿”å› None æˆ–ç©º
        return None

    @staticmethod
    async def get_cached_analysis_result():
        """åªè·å–ç¼“å­˜çš„åˆ†æç»“æœï¼Œä¸è§¦å‘æ–°çš„åˆ†æ"""
        ResourceService._auto_analysis_result = None
        ResourceService._auto_analysis_time = None
        
        try:
            cache_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), "cache", "auto_analysis_cache.json")
            if os.path.exists(cache_file):
                os.remove(cache_file)
                logger.info(f"Removed cache file: {cache_file}")
        except Exception as e:
            logger.error(f"Failed to remove cached analysis result: {e}")
        
        return None

    @staticmethod
    async def _analyze_with_ollama(folder_info: List[Dict]) -> Dict[str, List[Dict]]:
        """ä½¿ç”¨Ollamaæœ¬åœ°å¤§æ¨¡å‹åˆ†ææ–‡ä»¶å¤¹å¹¶ç”Ÿæˆäº”å¤§å›ºå®šåˆ†ç±»"""
        try:
            ollama_base_url = config.OLLAMA_BASE_URL
            ollama_model = config.OLLAMA_MODEL

            logger.info(f"Starting Ollama analysis with model: {ollama_model}, URL: {ollama_base_url}")

            # ä¿æŒå®Œæ•´çš„æ ·æœ¬æ•°é‡ï¼Œä¸å‡å°‘
            sample_size = min(100, len(folder_info))
            sample_folders = random.sample(folder_info, sample_size)
            logger.info(f"Processing {sample_size} sample files")

            # åˆ†æ‰¹å¤„ç†ä»¥æé«˜æ€§èƒ½ï¼Œä½†ä¿æŒå®Œæ•´åˆ†æ
            batch_size = 25  # æ¯æ‰¹å¤„ç†25ä¸ªæ–‡ä»¶ï¼Œå‡å°‘å•æ¬¡è¯·æ±‚çš„å¤æ‚åº¦
            all_categories = {"å­¦æœ¯è®ºæ–‡": [], "è°ƒæŸ¥æŠ¥å‘Š": [], "ä¸“ä¸šä¹¦ç±": [], "æ”¿ç­–æ–‡ä»¶": [], "æ³•è§„æ ‡å‡†": []}

            for batch_start in range(0, len(sample_folders), batch_size):
                batch_end = min(batch_start + batch_size, len(sample_folders))
                batch_folders = sample_folders[batch_start:batch_end]

                logger.info(f"Processing batch {batch_start//batch_size + 1}/{(len(sample_folders) + batch_size - 1)//batch_size}, files {batch_start}-{batch_end-1}")

                # ä¿æŒå®Œæ•´çš„promptï¼Œä¸ç®€åŒ–
                prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ–‡ä»¶åˆ†ç±»ä¸“å®¶ã€‚è¯·æ ¹æ®ä¸‹åˆ—æ–‡ä»¶çš„æ–‡ä»¶åï¼ˆå’Œè·¯å¾„ï¼‰ï¼Œå°†å®ƒä»¬ä¸¥æ ¼åˆ†ç±»åˆ°ä»¥ä¸‹äº”ä¸ªç±»åˆ«ä¹‹ä¸€ï¼š
1. å­¦æœ¯è®ºæ–‡ (Academic Paper)
2. è°ƒæŸ¥æŠ¥å‘Š (Survey Report)
3. ä¸“ä¸šä¹¦ç± (Professional Book)
4. æ”¿ç­–æ–‡ä»¶ (Policy Document)
5. æ³•è§„æ ‡å‡† (Regulation/Standard)

åªèƒ½ç”¨è¿™äº”ä¸ªç±»åˆ«ï¼Œä¸èƒ½æœ‰å…¶ä»–ç±»åˆ«ã€‚è¯·ç”¨å¦‚ä¸‹JSONæ ¼å¼è¿”å›ï¼š
{{
  "å­¦æœ¯è®ºæ–‡": [æ–‡ä»¶ç´¢å¼•åˆ—è¡¨],
  "è°ƒæŸ¥æŠ¥å‘Š": [æ–‡ä»¶ç´¢å¼•åˆ—è¡¨],
  "ä¸“ä¸šä¹¦ç±": [æ–‡ä»¶ç´¢å¼•åˆ—è¡¨],
  "æ”¿ç­–æ–‡ä»¶": [æ–‡ä»¶ç´¢å¼•åˆ—è¡¨],
  "æ³•è§„æ ‡å‡†": [æ–‡ä»¶ç´¢å¼•åˆ—è¡¨]
}}
æ–‡ä»¶åˆ—è¡¨å¦‚ä¸‹ï¼š
{json.dumps([{'index': i, 'name': f['name'], 'path': f['path']} for i, f in enumerate(batch_folders)], ensure_ascii=False, indent=2)}
"""

                # ä¼˜åŒ–Ollamaæ€§èƒ½çš„è¿æ¥è®¾ç½®
                connector = aiohttp.TCPConnector(
                    limit=5,  # å‡å°‘å¹¶å‘è¿æ¥æ•°
                    limit_per_host=5,
                    keepalive_timeout=600,
                    enable_cleanup_closed=True,
                    use_dns_cache=True,
                    ttl_dns_cache=300
                )

                # é’ˆå¯¹Ollamaä¼˜åŒ–çš„è¶…æ—¶è®¾ç½®
                timeout = aiohttp.ClientTimeout(
                    total=1800,      # 30åˆ†é’Ÿæ€»è¶…æ—¶
                    connect=60,      # 1åˆ†é’Ÿè¿æ¥è¶…æ—¶
                    sock_read=1200   # 20åˆ†é’Ÿè¯»å–è¶…æ—¶ï¼Œç»™æ¨¡å‹å……è¶³å¤„ç†æ—¶é—´
                )

                async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                    # ä¼˜åŒ–Ollamaæ€§èƒ½çš„payloadè®¾ç½® - å¯ç”¨æµå¼å¤„ç†
                    payload = {
                        "model": ollama_model,
                        "messages": [
                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡ä»¶åˆ†ç±»ä¸“å®¶ï¼Œåªèƒ½ç”¨äº”ä¸ªç±»åˆ«åˆ†ç±»ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚"},
                            {"role": "user", "content": prompt}
                        ],
                        "stream": True,  # å¯ç”¨æµå¼å¤„ç†
                        "options": {
                            "temperature": 0.1,      # é™ä½éšæœºæ€§ï¼Œæé«˜ä¸€è‡´æ€§å’Œé€Ÿåº¦
                            "top_p": 0.8,           # å‡å°‘å€™é€‰tokenï¼Œæé«˜é€Ÿåº¦
                            "top_k": 20,            # é™åˆ¶å€™é€‰æ•°é‡ï¼Œæé«˜é€Ÿåº¦
                            "repeat_penalty": 1.1,  # é¿å…é‡å¤ï¼Œæé«˜æ•ˆç‡
                            "num_predict": -1,      # ä¸é™åˆ¶è¾“å‡ºé•¿åº¦ï¼Œä¿è¯å®Œæ•´åˆ†æ
                            "num_ctx": 4096,        # è®¾ç½®åˆé€‚çš„ä¸Šä¸‹æ–‡é•¿åº¦
                            "num_thread": -1,       # ä½¿ç”¨æ‰€æœ‰å¯ç”¨CPUçº¿ç¨‹
                            "num_gpu": -1,          # ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
                            "low_vram": False       # å¦‚æœæ˜¾å­˜å……è¶³ï¼Œä¸å¯ç”¨ä½æ˜¾å­˜æ¨¡å¼
                        }
                    }

                    logger.info(f"Sending streaming batch request to Ollama: {ollama_base_url}/api/chat")

                    try:
                        async with session.post(
                            f"{ollama_base_url}/api/chat",
                            json=payload,
                            headers={'Content-Type': 'application/json'}
                        ) as response:
                            logger.info(f"Ollama streaming batch response status: {response.status}")

                            if response.status != 200:
                                response_text = await response.text()
                                logger.error(f"Ollama API error response: {response_text}")
                                raise Exception(f"Ollama API request failed with status {response.status}: {response_text}")

                            # å¼‚æ­¥æµå¼å¤„ç†å“åº”
                            model_response = ""
                            chunk_count = 0

                            async for line in response.content:
                                if line:
                                    try:
                                        line_str = line.decode('utf-8').strip()
                                        if line_str:
                                            # è§£ææ¯ä¸ªæµå¼å“åº”å—
                                            chunk_data = json.loads(line_str)
                                            if "message" in chunk_data and "content" in chunk_data["message"]:
                                                content_chunk = chunk_data["message"]["content"]
                                                model_response += content_chunk
                                                chunk_count += 1

                                                # æ¯æ”¶åˆ°10ä¸ªchunkè®°å½•ä¸€æ¬¡è¿›åº¦
                                                if chunk_count % 10 == 0:
                                                    logger.debug(f"Batch {batch_start//batch_size + 1} received {chunk_count} chunks, current length: {len(model_response)}")

                                            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                                            if chunk_data.get("done", False):
                                                logger.info(f"Batch {batch_start//batch_size + 1} streaming completed, total chunks: {chunk_count}, final length: {len(model_response)}")
                                                break

                                    except json.JSONDecodeError:
                                        # è·³è¿‡æ— æ•ˆçš„JSONè¡Œ
                                        continue
                                    except Exception as parse_error:
                                        logger.warning(f"Error parsing stream chunk: {parse_error}")
                                        continue

                    except asyncio.TimeoutError:
                        logger.error(f"Ollama streaming batch request timed out for batch {batch_start//batch_size + 1}")
                        raise Exception(f"Ollama batch request timed out")
                    except aiohttp.ClientError as client_error:
                        logger.error(f"Ollama client error in batch {batch_start//batch_size + 1}: {client_error}")
                        raise Exception(f"Ollama client error: {client_error}")
                    except Exception as req_error:
                        logger.error(f"Ollama request error in batch {batch_start//batch_size + 1}: {req_error}")
                        raise

                # è§£ææ‰¹æ¬¡å“åº”
                if not model_response.strip():
                    logger.warning(f"Empty response from Ollama for batch {batch_start//batch_size + 1}")
                    continue

                # æ›´å¼ºå¥çš„JSONæå–
                json_match = re.search(r'```json\s*([\s\S]*?)\s*```', model_response)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # å°è¯•æå–å¤§æ‹¬å·å†…å®¹
                    json_match = re.search(r'\{[\s\S]*\}', model_response)
                    if json_match:
                        json_str = json_match.group(0)
                    else:
                        json_str = model_response.strip()

                logger.info(f"Batch {batch_start//batch_size + 1} extracted JSON string length: {len(json_str)}")

                try:
                    batch_category_indices = json.loads(json_str)
                    logger.info(f"Batch {batch_start//batch_size + 1} successfully parsed categories")
                except json.JSONDecodeError as json_error:
                    logger.error(f"Batch {batch_start//batch_size + 1} JSON parsing failed: {json_error}")
                    # å°è¯•ä¿®æ­£JSONæ ¼å¼
                    try:
                        # æ¸…ç†å¯èƒ½çš„æ ¼å¼é—®é¢˜
                        cleaned_json = re.sub(r'[^\{\}\[\]\,\:\"\d\s\w\.\-\_\u4e00-\u9fa5]', '', json_str)
                        batch_category_indices = json.loads(cleaned_json)
                        logger.info(f"Batch {batch_start//batch_size + 1} successfully parsed cleaned JSON")
                    except:
                        logger.error(f"Batch {batch_start//batch_size + 1} failed to parse even cleaned JSON, skipping batch")
                        continue

                # å°†æ‰¹æ¬¡ç»“æœåˆå¹¶åˆ°æ€»ç»“æœä¸­
                fixed_categories = ["å­¦æœ¯è®ºæ–‡", "è°ƒæŸ¥æŠ¥å‘Š", "ä¸“ä¸šä¹¦ç±", "æ”¿ç­–æ–‡ä»¶", "æ³•è§„æ ‡å‡†"]

                for cat in fixed_categories:
                    indices = batch_category_indices.get(cat, [])
                    for idx in indices:
                        try:
                            idx_int = int(idx)
                            if 0 <= idx_int < len(batch_folders):
                                all_categories[cat].append(batch_folders[idx_int])
                        except (ValueError, TypeError):
                            continue

                logger.info(f"Batch {batch_start//batch_size + 1} completed, processed {len(batch_folders)} files")

                # æ·»åŠ æ‰¹æ¬¡é—´çš„çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¿‡è½½Ollama
                if batch_end < len(sample_folders):
                    await asyncio.sleep(1)

            # ç»Ÿè®¡æœ€ç»ˆç»“æœ
            total_assigned = sum(len(files) for files in all_categories.values())
            logger.info(f"Ollama analysis completed successfully, processed {len(sample_folders)} files in {(len(sample_folders) + batch_size - 1)//batch_size} batches, assigned {total_assigned} files")

            return all_categories

        except Exception as e:
            logger.error(f"Error in Ollama analysis: {e}", exc_info=True)
            raise

    @staticmethod
    async def _analyze_with_deepseek(folder_info: List[Dict]) -> Dict[str, List[Dict]]:
        """ä½¿ç”¨DeepSeekå¤§æ¨¡å‹åˆ†ææ–‡ä»¶å¤¹å¹¶ç”Ÿæˆäº”å¤§å›ºå®šåˆ†ç±»ï¼Œä¼˜å…ˆä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹"""
        # é¦–å…ˆå°è¯•ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹
        try:
            logger.info("Attempting to use Ollama local model for analysis")
            return await ResourceService._analyze_with_ollama(folder_info)
        except Exception as ollama_error:
            logger.warning(f"Ollama analysis failed: {ollama_error}, falling back to DeepSeek API")

        # å¦‚æœOllamaå¤±è´¥ï¼Œåˆ™ä½¿ç”¨DeepSeek API
        try:
            api_key = config.DEEPSEEK_API_KEY
            if not api_key:
                raise ValueError("DeepSeek API key not found")
            sample_size = min(100, len(folder_info))
            sample_folders = random.sample(folder_info, sample_size)
            # æ˜ç¡®è¦æ±‚åªèƒ½ç”¨äº”ä¸ªç±»åˆ«
            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ–‡ä»¶åˆ†ç±»ä¸“å®¶ã€‚è¯·æ ¹æ®ä¸‹åˆ—æ–‡ä»¶çš„æ–‡ä»¶åï¼ˆå’Œè·¯å¾„ï¼‰ï¼Œå°†å®ƒä»¬ä¸¥æ ¼åˆ†ç±»åˆ°ä»¥ä¸‹äº”ä¸ªç±»åˆ«ä¹‹ä¸€ï¼š
1. å­¦æœ¯è®ºæ–‡ (Academic Paper)
2. è°ƒæŸ¥æŠ¥å‘Š (Survey Report)
3. ä¸“ä¸šä¹¦ç± (Professional Book)
4. æ”¿ç­–æ–‡ä»¶ (Policy Document)
5. æ³•è§„æ ‡å‡† (Regulation/Standard)

åªèƒ½ç”¨è¿™äº”ä¸ªç±»åˆ«ï¼Œä¸èƒ½æœ‰å…¶ä»–ç±»åˆ«ã€‚è¯·ç”¨å¦‚ä¸‹JSONæ ¼å¼è¿”å›ï¼š
{{
  "å­¦æœ¯è®ºæ–‡": [æ–‡ä»¶ç´¢å¼•åˆ—è¡¨],
  "è°ƒæŸ¥æŠ¥å‘Š": [æ–‡ä»¶ç´¢å¼•åˆ—è¡¨],
  "ä¸“ä¸šä¹¦ç±": [æ–‡ä»¶ç´¢å¼•åˆ—è¡¨],
  "æ”¿ç­–æ–‡ä»¶": [æ–‡ä»¶ç´¢å¼•åˆ—è¡¨],
  "æ³•è§„æ ‡å‡†": [æ–‡ä»¶ç´¢å¼•åˆ—è¡¨]
}}
æ–‡ä»¶åˆ—è¡¨å¦‚ä¸‹ï¼š
{json.dumps([{'name': f['name'], 'path': f['path']} for f in sample_folders], ensure_ascii=False, indent=2)}
"""
            client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com/v1")

            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨æ¥é¿å…äº‹ä»¶å¾ªç¯é—®é¢˜
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(
                    client.chat.completions.create,
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡ä»¶åˆ†ç±»ä¸“å®¶ï¼Œåªèƒ½ç”¨äº”ä¸ªç±»åˆ«åˆ†ç±»ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.2
                )
                response = await asyncio.wrap_future(future)
            model_response = response.choices[0].message.content
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', model_response)
            json_str = json_match.group(1) if json_match else model_response
            try:
                category_indices = json.loads(json_str)
            except Exception:
                # å°è¯•ä¿®æ­£æ ¼å¼
                json_str = re.sub(r'[^\{\}\[\]\,\:\"\d\s\w\.\-\_\u4e00-\u9fa5]', '', json_str)
                category_indices = json.loads(json_str)
            # ä¿è¯äº”å¤§ç±»éƒ½å­˜åœ¨
            fixed_categories = ["å­¦æœ¯è®ºæ–‡", "è°ƒæŸ¥æŠ¥å‘Š", "ä¸“ä¸šä¹¦ç±", "æ”¿ç­–æ–‡ä»¶", "æ³•è§„æ ‡å‡†"]
            categories = {cat: [] for cat in fixed_categories}
            assigned_indices = set()
            for cat in fixed_categories:
                indices = category_indices.get(cat, [])
                for idx in indices:
                    try:
                        idx_int = int(idx)
                    except Exception:
                        continue
                    if 0 <= idx_int < len(sample_folders):
                        categories[cat].append(sample_folders[idx_int])
                        assigned_indices.add(idx_int)
            # åªä¿ç•™äº”å¤§ç±»ä¸­è¢«åˆ†é…çš„æ–‡ä»¶ï¼Œæœªåˆ†é…çš„æ–‡ä»¶ç›´æ¥ä¸¢å¼ƒ
            logger.info("DeepSeek API analysis completed successfully")
            return categories
        except Exception as e:
            logger.error(f"Error in DeepSeek analysis: {e}", exc_info=True)
            raise


