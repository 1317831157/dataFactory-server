import sys
import os
# ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨DeepSeek API
from openai import OpenAI
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.resource import ResourceItem
import pathlib
from typing import List, Dict, Tuple
import random
import json
import hashlib
import aiohttp
import asyncio
import logging
from typing import List, Dict, Any
from datetime import datetime, timedelta
import uuid
import time
from concurrent.futures import ThreadPoolExecutor
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AnalysisTask:
    def __init__(self, task_id):
        self.task_id = task_id
        self.status = "pending"  # pending, running, completed, failed
        self.progress = 0
        self.result = None
        self.error = None
        self.start_time = time.time()
        self.is_auto_analysis = False  # é»˜è®¤ä¸æ˜¯è‡ªåŠ¨åˆ†æä»»åŠ¡

class ResourceService:
    """èµ„æºæœåŠ¡ç±»"""
    
    # ç¼“å­˜åˆ†ç±»ç»“æœï¼Œé¿å…é¢‘ç¹è¯·æ±‚å¤§æ¨¡å‹API
    _cache = {}
    _cache_time = None
    _cache_duration = timedelta(hours=1)  # ç¼“å­˜æœ‰æ•ˆæœŸ1å°æ—¶
    
    # å­˜å‚¨ä»»åŠ¡çŠ¶æ€çš„å­—å…¸
    _analysis_tasks = {}
    
    # æ·»åŠ ç±»å˜é‡ç”¨äºå­˜å‚¨è‡ªåŠ¨åˆ†æçš„ç»“æœ
    _auto_analysis_result = None
    _auto_analysis_time = None
    _auto_analysis_running = False

    @staticmethod
    async def get_resource_data() -> list[ResourceItem]:
        """è·å–èµ„æºæ•°æ®åˆ—è¡¨"""
        # ç›´æ¥è¿”å›è‡ªåŠ¨åˆ†æç»“æœ
        auto_analysis_result = await ResourceService.get_auto_analysis_result()
        if auto_analysis_result:
            return auto_analysis_result
        
        # å¦‚æœè‡ªåŠ¨åˆ†æç»“æœä¸ºç©ºï¼Œåˆ™ä½¿ç”¨åŸæœ‰é€»è¾‘
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        now = datetime.now()
        if (ResourceService._cache and ResourceService._cache_time and 
            now - ResourceService._cache_time < ResourceService._cache_duration):
            logger.info("Using cached resource data")
            return ResourceService._cache
        
        # æ‰«æç›®å½•è·¯å¾„ (å¯ä»¥ä»é…ç½®æ–‡ä»¶è¯»å–)
        base_dir = os.path.join(os.path.expanduser("~"), "Documents")
        
        # æ”¶é›†æ–‡ä»¶å¤¹ä¿¡æ¯
        folder_info = await ResourceService._collect_folder_info(base_dir)
        
        # æ™ºèƒ½åˆ†ææ–‡ä»¶å¤¹å¹¶ç”Ÿæˆåˆ†ç±»
        categories = await ResourceService._analyze_and_categorize_folders(folder_info)
        
        # ç»Ÿè®¡å„ç±»èµ„æºæ•°é‡
        result = []
        for i, (category, folders) in enumerate(categories.items(), 1):
            # ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆä¸€ä¸ªç¨³å®šçš„é¢œè‰²
            color = ResourceService._generate_color(category)
            # ä¸ºæ¯ä¸ªåˆ†ç±»é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„å›¾æ ‡
            icon = ResourceService._select_icon(category)
            
            result.append(
                ResourceItem(
                    id=i,
                    name=category,
                    count=len(folders),
                    icon=icon,
                    color=color
                )
            )
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•èµ„æºï¼Œè¿”å›é»˜è®¤æ•°æ®
        if not result:
            result = [
                ResourceItem(id=1, name="æ–‡æ¡£", count=0, icon="ğŸ“„", color="#1890ff"),
                ResourceItem(id=2, name="å›¾åƒ", count=0, icon="ğŸ–¼ï¸", color="#52c41a"),
                ResourceItem(id=3, name="éŸ³é¢‘", count=0, icon="ğŸµ", color="#722ed1"),
                ResourceItem(id=4, name="è§†é¢‘", count=0, icon="ğŸ¬", color="#faad14"),
                ResourceItem(id=5, name="æ•°æ®", count=0, icon="ğŸ“Š", color="#13c2c2"),
            ]
        
        # æ›´æ–°ç¼“å­˜
        ResourceService._cache = result
        ResourceService._cache_time = now
        
        return result
    
    @staticmethod
    async def _collect_folder_info(base_dir: str) -> List[Dict]:
        """æ”¶é›†ç›®å½•ä¸­çš„æ–‡ä»¶å¤¹ä¿¡æ¯"""
        folder_info = []
        
        # å¦‚æœæ˜¯Cç›˜è·¯å¾„ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
        if base_dir.startswith("C:"):
            logger.info(f"Skipping C: drive path: {base_dir}")
            return folder_info
            
        try:
            # éå†ç›®å½•ï¼Œåªæ”¶é›†æ–‡ä»¶å¤¹
            for root, dirs, _ in os.walk(base_dir):
                for dir_name in dirs:
                    # è·³è¿‡éšè—ç›®å½•ã€ç³»ç»Ÿç›®å½•å’ŒCç›˜è·¯å¾„
                    if dir_name.startswith('.') or dir_name.startswith('$') or root.startswith("C:"):
                        continue
                    
                    dir_path = os.path.join(root, dir_name)
                    try:
                        # è·å–æ–‡ä»¶å¤¹ä¿¡æ¯
                        dir_stat = os.stat(dir_path)
                        
                        # è®¡ç®—æ–‡ä»¶å¤¹æ·±åº¦
                        depth = len(os.path.relpath(dir_path, base_dir).split(os.sep))
                        
                        # æ”¶é›†æ–‡ä»¶å¤¹ä¿¡æ¯
                        folder_info.append({
                            'name': dir_name,
                            'path': dir_path,
                            'depth': depth,
                            'modified': datetime.fromtimestamp(dir_stat.st_mtime).isoformat(),
                            'relative_path': os.path.relpath(dir_path, base_dir)
                        })
                    except (PermissionError, FileNotFoundError) as e:
                        logger.warning(f"Error accessing folder {dir_path}: {e}")
        except Exception as e:
            logger.error(f"Error scanning directory {base_dir}: {e}")
        
        return folder_info
    
    @staticmethod
    async def _analyze_and_categorize_folders(folder_info: List[Dict]) -> Dict[str, List[Dict]]:
        """åˆ†ææ–‡ä»¶å¤¹å¹¶ç”Ÿæˆåˆ†ç±»"""
        # ç›´æ¥ä½¿ç”¨åŸºäºæ–‡ä»¶å¤¹åç§°çš„æ™ºèƒ½åˆ†ç±»
        return ResourceService._smart_categorize_folders(folder_info)
    
    @staticmethod
    def _smart_categorize_folders(folder_info: List[Dict]) -> Dict[str, List[Dict]]:
        """åŸºäºæ–‡ä»¶å¤¹åç§°çš„æ™ºèƒ½åˆ†ç±»æ–¹æ³•"""
        categories = {}
        
        # åŠ¨æ€ç”Ÿæˆåˆ†ç±»è§„åˆ™ï¼Œè€Œä¸æ˜¯ä½¿ç”¨é¢„å®šä¹‰çš„åˆ†ç±»
        category_rules = {}
        
        # ä»æ–‡ä»¶å¤¹åç§°ä¸­æå–å…³é”®è¯
        keywords_freq = {}
        for folder in folder_info:
            folder_name = folder['name'].lower()
            # åˆ†å‰²æ–‡ä»¶å¤¹åç§°ä¸ºå•è¯
            words = re.findall(r'[a-zA-Z\u4e00-\u9fa5]+', folder_name)
            for word in words:
                if len(word) > 2:  # å¿½ç•¥å¤ªçŸ­çš„è¯
                    keywords_freq[word] = keywords_freq.get(word, 0) + 1
        
        # é€‰æ‹©å‡ºç°é¢‘ç‡è¾ƒé«˜çš„å…³é”®è¯ä½œä¸ºåˆ†ç±»ä¾æ®
        top_keywords = sorted(keywords_freq.items(), key=lambda x: x[1], reverse=True)[:20]
        
        # æ ¹æ®å…³é”®è¯ç”Ÿæˆåˆ†ç±»
        for keyword, _ in top_keywords:
            # é¿å…é‡å¤çš„åˆ†ç±»
            if any(keyword in existing_keywords for existing_keywords in category_rules.values()):
                continue
                
            # åˆ›å»ºæ–°çš„åˆ†ç±»
            category_name = f"{keyword}ç›¸å…³æ–‡ä»¶"
            category_rules[category_name] = [keyword]
            
            # å¦‚æœåˆ†ç±»æ•°é‡è¾¾åˆ°10ä¸ªï¼Œåœæ­¢æ·»åŠ 
            if len(category_rules) >= 5:
                break
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„åˆ†ç±»ï¼Œæ·»åŠ ä¸€äº›é€šç”¨åˆ†ç±»
        if len(category_rules) < 5:
            default_categories = {
                "æ–‡æ¡£èµ„æ–™": ["doc", "pdf", "txt", "æ–‡æ¡£", "èµ„æ–™"],
                "åª’ä½“æ–‡ä»¶": ["media", "photo", "video", "music", "åª’ä½“", "ç…§ç‰‡", "è§†é¢‘", "éŸ³ä¹"],
                "å¼€å‘é¡¹ç›®": ["code", "dev", "src", "ä»£ç ", "å¼€å‘", "æºç "],
                "ç³»ç»Ÿæ–‡ä»¶": ["system", "config", "ç³»ç»Ÿ", "é…ç½®"],
                "å…¶ä»–æ–‡ä»¶": ["other", "misc", "å…¶ä»–", "æ‚é¡¹"]
            }
            
            for category, keywords in default_categories.items():
                if category not in category_rules:
                    category_rules[category] = keywords
        
        # ä¸ºæ¯ä¸ªæ–‡ä»¶å¤¹åˆ†ç±»
        for folder in folder_info:
            folder_name = folder['name'].lower()
            folder_path = folder['relative_path'].lower()
            
            # å¯»æ‰¾æœ€ä½³åŒ¹é…çš„åˆ†ç±»
            best_category = None
            best_score = 0
            
            for category, keywords in category_rules.items():
                score = 0
                
                # æ£€æŸ¥æ–‡ä»¶å¤¹åç§°ä¸­çš„å…³é”®è¯
                for keyword in keywords:
                    if keyword in folder_name:
                        score += 3  # æ–‡ä»¶å¤¹åç§°åŒ¹é…æƒé‡æ›´é«˜
                    if keyword in folder_path:
                        score += 1  # è·¯å¾„åŒ¹é…æƒé‡è¾ƒä½
                
                # æ›´æ–°æœ€ä½³åŒ¹é…
                if score > best_score:
                    best_score = score
                    best_category = category
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„åˆ†ç±»ï¼Œæ ¹æ®æ–‡ä»¶å¤¹åç§°ç‰¹å¾åˆ¤æ–­
            if not best_category:
                best_category = ResourceService._classify_by_name_pattern(folder_name)
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰åˆ†ç±»ï¼Œå½’ä¸º"å…¶ä»–æ–‡ä»¶å¤¹"
            if not best_category:
                best_category = "å…¶ä»–æ–‡ä»¶å¤¹"
            
            # æ·»åŠ åˆ°åˆ†ç±»ä¸­
            if best_category not in categories:
                categories[best_category] = []
            categories[best_category].append(folder)
        
        # é™åˆ¶åˆ†ç±»æ•°é‡ï¼Œåˆå¹¶å°åˆ†ç±»
        if len(categories) > 8:
            # æŒ‰æ–‡ä»¶å¤¹æ•°é‡æ’åº
            sorted_categories = sorted(categories.items(), key=lambda x: len(x[1]), reverse=True)
            
            # ä¿ç•™å‰7ä¸ªæœ€å¤§çš„åˆ†ç±»
            main_categories = dict(sorted_categories[:7])
            
            # å°†å‰©ä½™çš„å°åˆ†ç±»åˆå¹¶ä¸º"å…¶ä»–æ–‡ä»¶å¤¹"
            other_folders = []
            for category, folders in sorted_categories[7:]:
                other_folders.extend(folders)
            
            if other_folders:
                main_categories["å…¶ä»–æ–‡ä»¶å¤¹"] = other_folders
            
            categories = main_categories
        
        return categories
    
    @staticmethod
    def _classify_by_name_pattern(folder_name: str) -> str:
        """æ ¹æ®æ–‡ä»¶å¤¹åç§°æ¨¡å¼è¿›è¡Œåˆ†ç±»"""
        # æ•°å­—å¼€å¤´çš„æ–‡ä»¶å¤¹ï¼ˆå¯èƒ½æ˜¯æ—¥æœŸæˆ–ç‰ˆæœ¬ï¼‰
        if re.match(r'^\d+', folder_name):
            return "ç‰ˆæœ¬æˆ–æ—¥æœŸæ–‡ä»¶å¤¹"
        
        # åŒ…å«æ—¥æœŸæ ¼å¼çš„æ–‡ä»¶å¤¹
        if re.search(r'\d{4}[-_]\d{2}[-_]\d{2}', folder_name):
            return "æ—¥æœŸæ–‡ä»¶å¤¹"
        
        # å…¨å¤§å†™çš„æ–‡ä»¶å¤¹ï¼ˆå¯èƒ½æ˜¯ç¼©å†™æˆ–ç³»ç»Ÿæ–‡ä»¶å¤¹ï¼‰
        if folder_name.isupper() and len(folder_name) <= 10:
            return "ç³»ç»Ÿæˆ–ç¼©å†™æ–‡ä»¶å¤¹"
        
        # åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ–‡ä»¶å¤¹
        if re.search(r'[._-]', folder_name):
            return "ç‰¹æ®Šæ ¼å¼æ–‡ä»¶å¤¹"
        
        # å¾ˆé•¿çš„æ–‡ä»¶å¤¹åï¼ˆå¯èƒ½æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„ï¼‰
        if len(folder_name) > 30:
            return "è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å¤¹"
        
        return None
    
    @staticmethod
    def _generate_color(category: str) -> str:
        """ä¸ºåˆ†ç±»ç”Ÿæˆä¸€ä¸ªç¨³å®šçš„é¢œè‰²"""
        # ä½¿ç”¨åˆ†ç±»åç§°çš„å“ˆå¸Œå€¼ç”Ÿæˆé¢œè‰²ï¼Œç¡®ä¿åŒä¸€åˆ†ç±»æ€»æ˜¯å¾—åˆ°ç›¸åŒçš„é¢œè‰²
        hash_obj = hashlib.md5(category.encode())
        hash_int = int(hash_obj.hexdigest(), 16)
        
        # ç”ŸæˆæŸ”å’Œçš„é¢œè‰²
        r = (hash_int & 0xFF0000) >> 16
        g = (hash_int & 0x00FF00) >> 8
        b = hash_int & 0x0000FF
        
        # ç¡®ä¿é¢œè‰²ä¸ä¼šå¤ªæš—
        r = max(r, 100)
        g = max(g, 100)
        b = max(b, 100)
        
        return f"#{r:02x}{g:02x}{b:02x}"
    
    @staticmethod
    def _select_icon(category: str) -> str:
        """ä¸ºåˆ†ç±»é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„å›¾æ ‡"""
        # å®šä¹‰åˆ†ç±»å’Œå›¾æ ‡çš„æ˜ å°„
        category_icons = {
            "å·¥ä½œæ–‡æ¡£": "ğŸ’¼",
            "å­¦ä¹ èµ„æ–™": "ğŸ“š",
            "ä¸ªäººæ–‡ä»¶": "ğŸ‘¤",
            "å¼€å‘é¡¹ç›®": "ğŸ’»",
            "åª’ä½“æ–‡ä»¶": "ğŸ¬",
            "ä¸‹è½½æ–‡ä»¶": "ğŸ“¥",
            "ç³»ç»Ÿæ–‡ä»¶": "âš™ï¸",
            "å¨±ä¹å†…å®¹": "ğŸ®",
            "è´¢åŠ¡èµ„æ–™": "ğŸ’°",
            "å¥åº·åŒ»ç–—": "ğŸ¥",
            "ç‰ˆæœ¬æˆ–æ—¥æœŸæ–‡ä»¶å¤¹": "ğŸ“…",
            "æ—¥æœŸæ–‡ä»¶å¤¹": "ğŸ“…",
            "ç³»ç»Ÿæˆ–ç¼©å†™æ–‡ä»¶å¤¹": "ğŸ”¤",
            "ç‰¹æ®Šæ ¼å¼æ–‡ä»¶å¤¹": "ğŸ“", 
            "è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å¤¹": "ğŸ¤–",
            "å…¶ä»–æ–‡ä»¶å¤¹": "ğŸ“"
        }
        
        # å°è¯•ç²¾ç¡®åŒ¹é…
        if category in category_icons:
            return category_icons[category]
        
        # å°è¯•éƒ¨åˆ†åŒ¹é…
        for key, icon in category_icons.items():
            if key in category or category in key:
                return icon
        
        # é»˜è®¤å›¾æ ‡
        return "ğŸ“"

    @staticmethod
    async def start_analysis_task(base_dir: str, file_list=None, options=None) -> str:
        """å¯åŠ¨å¼‚æ­¥åˆ†æä»»åŠ¡
        
        Args:
            base_dir: åŸºç¡€ç›®å½•è·¯å¾„
            file_list: æ–‡ä»¶åˆ—è¡¨ï¼Œå¦‚æœæä¾›åˆ™åªåˆ†æè¿™äº›æ–‡ä»¶
            options: å…¶ä»–é€‰é¡¹
        
        Returns:
            str: ä»»åŠ¡ID
        """
        # ç”Ÿæˆå”¯ä¸€ä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # å¦‚æœç›®å½•ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ç›®å½•
        if not base_dir or not os.path.exists(base_dir):
            base_dir = os.path.join(os.path.expanduser("~"), "Documents")
        
        # åˆ›å»ºä»»åŠ¡å¯¹è±¡
        task = AnalysisTask(task_id)
        # ä½¿ç”¨ç±»å˜é‡å­˜å‚¨ä»»åŠ¡
        ResourceService._analysis_tasks[task_id] = task
        
        # å¯åŠ¨å¼‚æ­¥ä»»åŠ¡
        asyncio.create_task(ResourceService._run_analysis_task(task, base_dir, file_list, options))
        
        return task_id

    @staticmethod
    async def get_task_status(task_id: str) -> Dict:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        logger.info(f"Getting task status for task_id: {task_id}")
        logger.info(f"Available tasks: {list(ResourceService._analysis_tasks.keys())}")
        
        task = ResourceService._analysis_tasks.get(task_id)
        if not task:
            logger.warning(f"Task not found: {task_id}")
            return {"status": "not_found"}
        
        result = {
            "status": task.status,
            "progress": task.progress
        }
        
        # å¦‚æœä»»åŠ¡å®Œæˆï¼Œè¿”å›ç»“æœ
        if task.status == "completed":
            result["result"] = task.result
        elif task.status == "failed":
            result["error"] = str(task.error)
        
        # è®¡ç®—è¿è¡Œæ—¶é—´
        result["elapsed_time"] = time.time() - task.start_time
        
        logger.info(f"Task status: {result}")
        return result

    @staticmethod
    async def _run_analysis_task(task: AnalysisTask, base_dir: str, file_list=None, options=None):
        """è¿è¡Œåˆ†æä»»åŠ¡"""
        try:
            task.status = "running"
            task.progress = 5
            
            if file_list:
                logger.info(f"Using provided file list with {len(file_list)} files")
                
                # å¤„ç†æ–‡ä»¶åˆ—è¡¨ï¼ŒæŒ‰æ–‡ä»¶å¤¹åˆ†ç»„
                folder_structure = {}
                for file in file_list:
                    path = file["path"]
                    # æå–æ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„
                    folder_path = os.path.dirname(path)
                    
                    if not folder_path:
                        folder_path = "æ ¹ç›®å½•"
                    
                    if folder_path not in folder_structure:
                        folder_structure[folder_path] = []
                    
                    folder_structure[folder_path].append(file)
                
                # ç›´æ¥ä½¿ç”¨æ–‡ä»¶å¤¹åç§°ä½œä¸ºåˆ†ç±»
                categories = {}
                for folder_path, files in folder_structure.items():
                    folder_name = os.path.basename(folder_path)
                    if not folder_name:
                        folder_name = os.path.basename(os.path.dirname(folder_path))
                    if not folder_name:
                        folder_name = "å…¶ä»–"
                    
                    if folder_name not in categories:
                        categories[folder_name] = []
                    
                    categories[folder_name].extend(files)
                
                task.progress = 90
            else:
                # æ”¶é›†æ–‡ä»¶å¤¹ä¿¡æ¯
                folder_info = await ResourceService._collect_folder_info(base_dir)
                task.progress = 50
                
                # æ™ºèƒ½åˆ†ææ–‡ä»¶å¤¹å¹¶ç”Ÿæˆåˆ†ç±»
                categories = ResourceService._smart_categorize_folders(folder_info)
                task.progress = 90
            
            # æ„å»ºæœ€ç»ˆç»“æœ
            result = []
            for category, items in categories.items():
                result.append({
                    "id": len(result) + 1,
                    "name": category,
                    "count": len(items),
                    "icon": ResourceService._select_icon(category),
                    "color": ResourceService._generate_color(category),
                    "folders": items if not file_list else []  # å¦‚æœæ˜¯æ–‡ä»¶åˆ—è¡¨åˆ†æï¼Œä¸è¿”å›æ–‡ä»¶å¤¹
                })
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = "completed"
            task.progress = 100
            task.result = result
            
            # æ›´æ–°ç¼“å­˜
            ResourceService._cache = result
            ResourceService._cache_time = datetime.now()
            
        except Exception as e:
            logger.error(f"Analysis task failed: {e}")
            task.status = "failed"
            task.error = e

    @staticmethod
    async def auto_analyze_local_directories():
        """è‡ªåŠ¨åˆ†ææœ¬åœ°æ–‡ä»¶å¤¹"""
        print("è‡ªåŠ¨åˆ†ææœ¬åœ°æ–‡ä»¶å¤¹",ResourceService._auto_analysis_running)
        # é˜²æ­¢é‡å¤è¿è¡Œ
        if ResourceService._auto_analysis_running:
            logger.info("Auto analysis already running, skipping")
            return
        
        try:
            # è®¾ç½®è¿è¡Œæ ‡å¿—
            ResourceService._auto_analysis_running = True
            logger.info("Starting automatic analysis of local directories")
            
            # è·å–ç”¨æˆ·ä¸»ç›®å½•
            home_dir = os.path.expanduser("~")
            
            # ç›´æ¥æ·»åŠ Dç›˜ã€Eç›˜ã€Fç›˜ç­‰éCç›˜ç›®å½•
            drive_dirs = []
            for drive_letter in "DEFGHIJKLMNOPQRSTUVWXYZ":
                drive_path = f"{drive_letter}:\\"
                if os.path.exists(drive_path):
                    logger.info(f"Found drive: {drive_path}")
                    drive_dirs.append(drive_path)
            
            # å¦‚æœæ‰¾åˆ°äº†å…¶ä»–é©±åŠ¨å™¨ï¼Œä½¿ç”¨å®ƒä»¬ä»£æ›¿home_dir
            if drive_dirs:
                logger.info(f"Using drives: {drive_dirs} instead of home directory")
                common_dirs = []  # åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                
                # ä¸ºæ¯ä¸ªé©±åŠ¨å™¨æ”¶é›†ç›®å½•
                for drive in drive_dirs:
                    try:
                        # æ·»åŠ é©±åŠ¨å™¨æ ¹ç›®å½•
                        common_dirs.append(drive)
                        
                        # æ·»åŠ ç¬¬ä¸€å±‚ç›®å½•
                        with os.scandir(drive) as entries:
                            for entry in entries:
                                if entry.is_dir() and not entry.name.startswith('.'):
                                    common_dirs.append(entry.path)
                    except Exception as e:
                        logger.error(f"Error scanning drive {drive}: {e}")
            else:
                logger.info(f"No additional drives found, using home directory: {home_dir}")
                common_dirs = []  # åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                
                # ä¸ä½¿ç”¨é¢„å®šä¹‰ç›®å½•ï¼Œè€Œæ˜¯åŠ¨æ€è·å–ç”¨æˆ·ç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•ï¼ˆæœ€å¤§3å±‚ï¼‰
                try:
                    # æ·»åŠ ç¬¬ä¸€å±‚ç›®å½•ï¼Œä½†è·³è¿‡Cç›˜è·¯å¾„
                    with os.scandir(home_dir) as entries:
                        for entry in entries:
                            if entry.is_dir() and not entry.name.startswith('.'):
                                # è·³è¿‡Cç›˜è·¯å¾„
                                if entry.path.startswith("C:"):
                                    logger.info(f"Skipping C: drive path: {entry.path}")
                                    continue
                                common_dirs.append(entry.path)
                except Exception as e:
                    logger.error(f"Error scanning home directory: {e}")
            
            # æ”¶é›†å­˜åœ¨çš„ç›®å½•ï¼Œæ’é™¤Cç›˜è·¯å¾„
            existing_dirs = [d for d in common_dirs if os.path.exists(d) and os.path.isdir(d) and not d.startswith("C:")]
            logger.info(f"After filtering, {len(existing_dirs)} directories remain")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç›®å½•ï¼Œæ·»åŠ ç”¨æˆ·ä¸»ç›®å½•ï¼ˆå¦‚æœä¸åœ¨Cç›˜ï¼‰
            if not existing_dirs and not home_dir.startswith("C:"):
                existing_dirs.append(home_dir)
                logger.info(f"No directories found, using home directory: {home_dir}")
            
            # å¦‚æœä»ç„¶æ²¡æœ‰ç›®å½•ï¼Œåˆ›å»ºä¸€ä¸ªç©ºç»“æœ
            if not existing_dirs:
                logger.warning("No valid directories found for analysis")
                # åˆ›å»ºä¸€ä¸ªç©ºç»“æœ
                ResourceService._auto_analysis_result = []
                ResourceService._auto_analysis_time = datetime.now()
                ResourceService._auto_analysis_running = False
                return
            
            # åˆ›å»ºåˆ†æä»»åŠ¡
            task_id = str(uuid.uuid4())
            task = AnalysisTask(task_id)
            task.is_auto_analysis = True  # æ˜ç¡®æ ‡è®°ä¸ºè‡ªåŠ¨åˆ†æä»»åŠ¡
            ResourceService._analysis_tasks[task_id] = task
            logger.info(f"Created auto analysis task with ID: {task_id}")
            # logger.info(f"existing_dirs: {existing_dirs}")
            # è¿è¡Œåˆ†æä»»åŠ¡
            await ResourceService._run_auto_analysis(task, existing_dirs)
            
            # å­˜å‚¨ç»“æœ
            if task.status == "completed":
                # ç¼“å­˜åˆ†æç»“æœ
                ResourceService._auto_analysis_result = task.result
                ResourceService._auto_analysis_time = datetime.now()
                
                # å°†ç»“æœä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œä»¥ä¾¿æœåŠ¡é‡å¯åä»èƒ½ä½¿ç”¨
                try:
                    cache_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "cache")
                    os.makedirs(cache_dir, exist_ok=True)
                    
                    cache_file = os.path.join(cache_dir, "auto_analysis_cache.json")
                    with open(cache_file, "w", encoding="utf-8") as f:
                        json.dump({
                            "result": task.result,
                            "timestamp": datetime.now().isoformat()
                        }, f, ensure_ascii=False, indent=2)
                    
                    logger.info(f"Cached analysis result to {cache_file}")
                except Exception as e:
                    logger.error(f"Failed to cache analysis result: {e}")
                
                logger.info(f"Automatic analysis completed with {len(task.result)} categories")
            else:
                logger.error(f"Automatic analysis failed: {task.error}")
        
        except Exception as e:
            logger.error(f"Error in automatic analysis: {e}")
        finally:
            # ç¡®ä¿åœ¨å‡½æ•°ç»“æŸæ—¶é‡ç½®è¿è¡Œæ ‡å¿—
            ResourceService._auto_analysis_running = False
            logger.info("Auto analysis completed, reset running flag to False")

    @staticmethod
    async def _run_auto_analysis(task: AnalysisTask, directories: List[str]):
        """è¿è¡Œè‡ªåŠ¨åˆ†æä»»åŠ¡ - åŸºäºæ–‡ä»¶å¤¹è¿›è¡Œæ™ºèƒ½åˆ†ç±»"""
        try:
            task.status = "running"
            task.progress = 5
            
            # æ£€æŸ¥ç›®å½•åˆ—è¡¨æ˜¯å¦ä¸ºç©º
            if not directories:
                logger.warning("No directories to analyze")
                task.status = "completed"
                task.progress = 100
                task.result = []  # è¿”å›ç©ºç»“æœ
                return
            
            # æ”¶é›†æ‰€æœ‰ç›®å½•ä¸‹çš„æ–‡ä»¶å¤¹ä¿¡æ¯
            all_folder_info = []
            
            for i, directory in enumerate(directories):
                try:
                    logger.info(f"Processing directory {i+1}/{len(directories)}: {directory}")
                    # ä½¿ç”¨å¹¶è¡Œå¤„ç†æ›¿ä»£å•çº¿ç¨‹æ”¶é›†æ–‡ä»¶å¤¹ä¿¡æ¯
                    # åˆ›å»ºä¸€ä¸ªæœ‰é™æ·±åº¦çš„æ–‡ä»¶å¤¹æ”¶é›†å‡½æ•°
                    folder_info = await ResourceService._collect_folder_info_limited(directory, max_depth=2)
                    all_folder_info.extend(folder_info)
                    
                    # æ›´æ–°è¿›åº¦
                    task.progress = 5 + int((i + 1) / len(directories) * 50)
                    logger.info(f"Collected {len(folder_info)} folders from {directory}")
                except Exception as e:
                    logger.error(f"Error processing directory {directory}: {e}")
                    continue
            
            logger.info(f"Total folders collected: {len(all_folder_info)}")
            
            # å¦‚æœæ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•æ–‡ä»¶å¤¹ä¿¡æ¯ï¼Œè¿”å›ç©ºç»“æœ
            if not all_folder_info:
                logger.warning("No folder information collected")
                task.status = "completed"
                task.progress = 100
                task.result = []
                return
            
            # ä¼˜å…ˆä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ–‡ä»¶å¤¹å¹¶ç”Ÿæˆåˆ†ç±»
            try:
                # å°è¯•ä½¿ç”¨DeepSeekå¤§æ¨¡å‹åˆ†æ
                categories = await ResourceService._analyze_with_deepseek(all_folder_info)
                logger.info("Successfully used DeepSeek model for folder categorization")
            except Exception as e:
                # å¦‚æœå¤§æ¨¡å‹åˆ†æå¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰çš„åˆ†ææ–¹æ³•
                logger.warning(f"DeepSeek analysis failed: {e}, falling back to basic categorization")
                categories = ResourceService._smart_categorize_folders(all_folder_info)
            task.progress = 90
            
            # æ„å»ºæœ€ç»ˆç»“æœ
            result = []
            for category, folders in categories.items():
                result.append({
                    "id": len(result) + 1,
                    "name": category,
                    "count": len(folders),
                    "icon": ResourceService._select_icon(category),
                    "color": ResourceService._generate_color(category),
                    "folders": folders[:50]  # é™åˆ¶æ–‡ä»¶å¤¹æ•°é‡ï¼Œé¿å…è¿”å›è¿‡å¤šæ•°æ®
                })
            
            logger.info(f'Auto analysis result: {len(result)} categories')
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = "completed"
            task.progress = 100
            task.result = result
        except Exception as e:
            logger.error(f"Auto analysis task failed: {e}")
            task.status = "failed"
            task.error = str(e)

    @staticmethod
    async def get_auto_analysis_result():
        """è·å–è‡ªåŠ¨åˆ†æç»“æœ"""
        # å¦‚æœå†…å­˜ä¸­æœ‰ç»“æœä¸”æœªè¿‡æœŸï¼Œç›´æ¥è¿”å›
        if (ResourceService._auto_analysis_result is not None and 
            ResourceService._auto_analysis_time is not None and
            datetime.now() - ResourceService._auto_analysis_time < timedelta(hours=24)):
            logger.info("Using in-memory auto analysis result")
            return ResourceService._auto_analysis_result
        
        # å¦‚æœå†…å­˜ä¸­æ²¡æœ‰ç»“æœæˆ–å·²è¿‡æœŸï¼Œå°è¯•ä»ç¼“å­˜æ–‡ä»¶åŠ è½½
        try:
            cache_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), "cache", "auto_analysis_cache.json")
            if os.path.exists(cache_file):
                with open(cache_file, "r", encoding="utf-8") as f:
                    cache_data = json.load(f)
                
                # ä¿®å¤è¿™ä¸€è¡Œï¼Œä½¿ç”¨æ­£ç¡®çš„logger.erroræ ¼å¼
                logger.info(f"ResourceService auto analysis result: {ResourceService._auto_analysis_result}")
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
                cache_time = datetime.fromisoformat(cache_data["timestamp"])
                if datetime.now() - cache_time < timedelta(hours=24):
                    logger.info("Loaded auto analysis result from cache file")
                    ResourceService._auto_analysis_result = cache_data["result"]
                    ResourceService._auto_analysis_time = cache_time
                    return ResourceService._auto_analysis_result
        except Exception as e:
            logger.error(f"Failed to load cached analysis result: {e}")
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ç¼“å­˜ï¼Œé‡æ–°åˆ†æ
        logger.info("No valid cache found, starting new auto analysis")
        await ResourceService.auto_analyze_local_directories()
        
        return ResourceService._auto_analysis_result

    @staticmethod
    async def get_cached_analysis_result():
        """åªè·å–ç¼“å­˜çš„åˆ†æç»“æœï¼Œä¸è§¦å‘æ–°çš„åˆ†æ"""
        # æ¸…ç©ºå†…å­˜ç¼“å­˜
        ResourceService._auto_analysis_result = None
        ResourceService._auto_analysis_time = None
        
        # æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
        try:
            cache_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), "cache", "auto_analysis_cache.json")
            if os.path.exists(cache_file):
                # æ¸…ç©ºæ–‡ä»¶ç¼“å­˜
                os.remove(cache_file)
                logger.info(f"Removed cache file: {cache_file}")
                return None
        except Exception as e:
            logger.error(f"Failed to remove cached analysis result: {e}")
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ç¼“å­˜ï¼Œè¿”å›None
        return None

    @staticmethod
    async def _collect_folder_info_limited(base_dir: str, max_depth: int = 2) -> List[Dict]:
        """æ”¶é›†ç›®å½•ä¸­çš„æ–‡ä»¶å¤¹ä¿¡æ¯ï¼Œé™åˆ¶æ·±åº¦ä»¥æé«˜æ€§èƒ½"""
        folder_info = []
        
        # å¦‚æœæ˜¯Cç›˜è·¯å¾„ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
        if base_dir.startswith("C:"):
            logger.info(f"Skipping C: drive path: {base_dir}")
            return folder_info
        
        # ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œä»¥é¿å…é˜»å¡
        loop = asyncio.get_event_loop()
        
        def scan_directory(dir_path, current_depth=0):
            """é€’å½’æ‰«æç›®å½•ï¼Œä½†é™åˆ¶æ·±åº¦"""
            results = []
            
            if current_depth > max_depth:
                return results
            
            try:
                # ä½¿ç”¨os.scandiræ›´é«˜æ•ˆåœ°æ‰«æç›®å½•
                with os.scandir(dir_path) as entries:
                    for entry in entries:
                        try:
                            # åªå¤„ç†ç›®å½•
                            if entry.is_dir():
                                # è·³è¿‡éšè—ç›®å½•å’Œç³»ç»Ÿç›®å½•
                                if entry.name.startswith('.') or entry.name.startswith('$'):
                                    continue
                                
                                # è·å–æ–‡ä»¶å¤¹ä¿¡æ¯
                                dir_stat = entry.stat()
                                
                                # è®¡ç®—æ–‡ä»¶å¤¹æ·±åº¦
                                depth = current_depth + 1
                                
                                # æ”¶é›†æ–‡ä»¶å¤¹ä¿¡æ¯
                                results.append({
                                    'name': entry.name,
                                    'path': entry.path,
                                    'depth': depth,
                                    'modified': datetime.fromtimestamp(dir_stat.st_mtime).isoformat(),
                                    'relative_path': os.path.relpath(entry.path, base_dir)
                                })
                                
                                # å¦‚æœæ·±åº¦æœªè¾¾åˆ°æœ€å¤§å€¼ï¼Œç»§ç»­é€’å½’
                                if depth < max_depth:
                                    results.extend(scan_directory(entry.path, depth))
                        except (PermissionError, FileNotFoundError) as e:
                            # å¿½ç•¥æƒé™é”™è¯¯å’Œæ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯
                            continue
                        except Exception as e:
                            logger.warning(f"Error processing entry {entry.path}: {e}")
            except Exception as e:
                logger.warning(f"Error scanning directory {dir_path}: {e}")
            
            return results
        
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œç›®å½•æ‰«æ
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒIOå¯†é›†å‹æ“ä½œ
            with ThreadPoolExecutor(max_workers=min(32, (os.cpu_count() or 1) * 4)) as executor:
                folder_info = await loop.run_in_executor(executor, scan_directory, base_dir)
        except Exception as e:
            logger.error(f"Error in folder collection for {base_dir}: {e}")
        
        return folder_info

    @staticmethod
    async def _analyze_with_deepseek(folder_info: List[Dict]) -> Dict[str, List[Dict]]:
        """ä½¿ç”¨DeepSeekå¤§æ¨¡å‹åˆ†ææ–‡ä»¶å¤¹å¹¶ç”Ÿæˆåˆ†ç±»"""
        try:
            # DeepSeek APIé…ç½®
            api_url = "https://api.deepseek.com/v1"
            # api_key = os.environ.get("DEEPSEEK_API_KEY")
            api_key ='sk-0c98c2a93954490aab152eeec9da1601'
            if not api_key:
                logger.error("DeepSeek API key not found in environment variables")
                raise ValueError("DeepSeek API key not found")
            
            # é™åˆ¶æ ·æœ¬æ•°é‡ï¼Œé¿å…è¯·æ±‚è¿‡å¤§
            sample_size = min(100, len(folder_info))
            sample_folders = random.sample(folder_info, sample_size) if len(folder_info) > sample_size else folder_info
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹æ–‡ä»¶å¤¹åˆ—è¡¨ï¼Œå¹¶æ ¹æ®æ–‡ä»¶å¤¹åç§°å’Œè·¯å¾„å°†å®ƒä»¬åˆ†ç±»åˆ°é€‚å½“çš„ç±»åˆ«ä¸­ã€‚
            è¯·åˆ›å»ºæœ‰æ„ä¹‰çš„ç±»åˆ«ï¼Œè¿™äº›ç±»åˆ«åº”è¯¥åæ˜ æ–‡ä»¶å¤¹çš„å®é™…å†…å®¹å’Œç”¨é€”ã€‚
            ä¾‹å¦‚ï¼Œ"é¡¹ç›®æ–‡æ¡£"ã€"å­¦ä¹ èµ„æ–™"ã€"å·¥ä½œæ–‡ä»¶"ç­‰ã€‚

            æ–‡ä»¶å¤¹åˆ—è¡¨:
            {json.dumps([{'name': f['name'], 'path': f['path']} for f in sample_folders], ensure_ascii=False, indent=2)}

            è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†ç±»ç»“æœï¼Œæ ¼å¼ä¸º:
            {{
                "ç±»åˆ«1": [æ–‡ä»¶å¤¹ç´¢å¼•åˆ—è¡¨],
                "ç±»åˆ«2": [æ–‡ä»¶å¤¹ç´¢å¼•åˆ—è¡¨],
                ...
            }}
            å…¶ä¸­æ–‡ä»¶å¤¹ç´¢å¼•æ˜¯æ–‡ä»¶å¤¹åœ¨æä¾›çš„åˆ—è¡¨ä¸­çš„ä½ç½®ï¼ˆä»0å¼€å§‹ï¼‰ã€‚

            è¯·ç¡®ä¿åˆ›å»ºçš„ç±»åˆ«æ•°é‡åœ¨3-7ä¸ªä¹‹é—´ï¼Œå¹¶ä¸”æ¯ä¸ªç±»åˆ«éƒ½æœ‰æ˜ç¡®çš„ä¸»é¢˜ã€‚
            """
            
            # å‡†å¤‡è¯·æ±‚æ•°æ®
            request_data = {
                "model": "deepseek-chat",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»åŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®æ–‡ä»¶å¤¹åç§°å’Œè·¯å¾„å¯¹æ–‡ä»¶å¤¹è¿›è¡Œè¯­ä¹‰åˆ†ç±»ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.2
            }
            
            # åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
            client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com/v1")
            
            # å‘é€è¯·æ±‚
            try:
                response = client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»åŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®æ–‡ä»¶å¤¹åç§°å’Œè·¯å¾„å¯¹æ–‡ä»¶å¤¹è¿›è¡Œè¯­ä¹‰åˆ†ç±»ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.2,
                    stream=False
                )
                
                # æå–æ¨¡å‹å›å¤
                model_response = response.choices[0].message.content
                print('model_response',model_response)
            except Exception as e:
                logger.error(f"Error in DeepSeek analysis: {e}")
                raise

            # ä»å›å¤ä¸­æå–JSONéƒ¨åˆ†
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', model_response)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = model_response
            
            # å°è¯•è§£æJSON
            try:
                # æ¸…ç†JSONå­—ç¬¦ä¸²ï¼Œç§»é™¤å¯èƒ½çš„éJSONå†…å®¹
                json_str = re.sub(r'[^\{\}\[\]\,\:\"\d\s\w\.\-\_\u4e00-\u9fa5]', '', json_str)
                category_indices = json.loads(json_str)
            except json.JSONDecodeError:
                logger.error(f"Failed to parse JSON from model response: {model_response}")
                raise Exception("Failed to parse JSON from model response")
            
            # å°†ç´¢å¼•æ˜ å°„å›æ–‡ä»¶å¤¹å¯¹è±¡
            categories = {}
            for category, indices in category_indices.items():
                categories[category] = []
                for idx in indices:
                    if 0 <= idx < len(sample_folders):
                        # æ‰¾åˆ°åŸå§‹æ–‡ä»¶å¤¹å¯¹è±¡
                        folder_name = sample_folders[idx]['name']
                        # å°†æ‰€æœ‰å…·æœ‰ç›¸åŒæˆ–ç›¸ä¼¼åç§°çš„æ–‡ä»¶å¤¹æ·»åŠ åˆ°æ­¤ç±»åˆ«
                        for folder in folder_info:
                            if folder['name'] == folder_name or folder['name'].lower() in folder_name.lower() or folder_name.lower() in folder['name'].lower():
                                categories[category].append(folder)
            
            # å¤„ç†æœªåˆ†ç±»çš„æ–‡ä»¶å¤¹
            all_categorized_folders = set()
            for folders in categories.values():
                for folder in folders:
                    all_categorized_folders.add(folder['path'])
            
            uncategorized = []
            for folder in folder_info:
                if folder['path'] not in all_categorized_folders:
                    uncategorized.append(folder)
            
            # å¦‚æœæœ‰æœªåˆ†ç±»çš„æ–‡ä»¶å¤¹ï¼Œæ·»åŠ "å…¶ä»–æ–‡ä»¶å¤¹"ç±»åˆ«
            if uncategorized:
                categories["å…¶ä»–æ–‡ä»¶å¤¹"] = uncategorized
            
            return categories
        except Exception as e:
            logger.error(f"Error in DeepSeek analysis: {e}")
            raise
