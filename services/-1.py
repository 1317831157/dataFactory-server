from models.resource import ResourceItem
import os
import pathlib
from typing import List, Dict, Tuple
import random
import json
import hashlib
import aiohttp
import asyncio
import logging
from typing import List, Dict, Any
from datetime import datetime, timedelta
import uuid
import time
from concurrent.futures import ThreadPoolExecutor

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å­˜å‚¨ä»»åŠ¡çŠ¶æ€çš„å­—å…¸
# _analysis_tasks = {}

class AnalysisTask:
    def __init__(self, task_id):
        self.task_id = task_id
        self.status = "pending"  # pending, running, completed, failed
        self.progress = 0
        self.result = None
        self.error = None
        self.start_time = time.time()

class ResourceService:
    """èµ„æºæœåŠ¡ç±»"""
    
    # ç¼“å­˜åˆ†ç±»ç»“æœï¼Œé¿å…é¢‘ç¹è¯·æ±‚å¤§æ¨¡å‹API
    _cache = {}
    _cache_time = None
    _cache_duration = timedelta(hours=1)  # ç¼“å­˜æœ‰æ•ˆæœŸ1å°æ—¶
    
    # å­˜å‚¨ä»»åŠ¡çŠ¶æ€çš„å­—å…¸
    _analysis_tasks = {}
    
    # æ·»åŠ ç±»å˜é‡ç”¨äºå­˜å‚¨è‡ªåŠ¨åˆ†æçš„ç»“æœ
    _auto_analysis_result = None
    _auto_analysis_time = None
    _auto_analysis_running = False

    @staticmethod
    async def get_resource_data() -> list[ResourceItem]:
        """è·å–èµ„æºæ•°æ®åˆ—è¡¨"""
        # ç›´æ¥è¿”å›è‡ªåŠ¨åˆ†æç»“æœ
        auto_analysis_result = await ResourceService.get_auto_analysis_result()
        if auto_analysis_result:
            return auto_analysis_result
        
        # å¦‚æœè‡ªåŠ¨åˆ†æç»“æœä¸ºç©ºï¼Œåˆ™ä½¿ç”¨åŸæœ‰é€»è¾‘
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        now = datetime.now()
        if (ResourceService._cache and ResourceService._cache_time and 
            now - ResourceService._cache_time < ResourceService._cache_duration):
            logger.info("Using cached resource data")
            return ResourceService._cache
        
        # æ‰«æç›®å½•è·¯å¾„ (å¯ä»¥ä»é…ç½®æ–‡ä»¶è¯»å–)
        base_dir = os.path.join(os.path.expanduser("~"), "Documents")
        
        # æ”¶é›†æ–‡ä»¶ä¿¡æ¯
        file_info = await ResourceService._collect_file_info(base_dir)
        
        # ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ–‡ä»¶å¹¶ç”Ÿæˆåˆ†ç±»
        categories = await ResourceService._analyze_and_categorize(file_info)
        
        # ç»Ÿè®¡å„ç±»èµ„æºæ•°é‡
        result = []
        for i, (category, files) in enumerate(categories.items(), 1):
            # ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆä¸€ä¸ªç¨³å®šçš„é¢œè‰²
            color = ResourceService._generate_color(category)
            # ä¸ºæ¯ä¸ªåˆ†ç±»é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„å›¾æ ‡
            icon = ResourceService._select_icon(category)
            
            result.append(
                ResourceItem(
                    id=i,
                    name=category,
                    count=len(files),
                    icon=icon,
                    color=color
                )
            )
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•èµ„æºï¼Œè¿”å›é»˜è®¤æ•°æ®
        if not result:
            result = [
                ResourceItem(id=1, name="æ–‡æ¡£", count=0, icon="ğŸ“„", color="#1890ff"),
                ResourceItem(id=2, name="å›¾åƒ", count=0, icon="ğŸ–¼ï¸", color="#52c41a"),
                ResourceItem(id=3, name="éŸ³é¢‘", count=0, icon="ğŸµ", color="#722ed1"),
                ResourceItem(id=4, name="è§†é¢‘", count=0, icon="ğŸ¬", color="#faad14"),
                ResourceItem(id=5, name="æ•°æ®", count=0, icon="ğŸ“Š", color="#13c2c2"),
            ]
        
        # æ›´æ–°ç¼“å­˜
        ResourceService._cache = result
        ResourceService._cache_time = now
        
        return result
    
    @staticmethod
    async def _collect_file_info(base_dir: str) -> List[Dict]:
        """æ”¶é›†ç›®å½•ä¸­çš„æ–‡ä»¶ä¿¡æ¯"""
        file_info = []
        
        # å®šä¹‰è¦æ”¶é›†çš„æ–‡ä»¶æ‰©å±•å
        valid_extensions = {
            # æ–‡æ¡£
            '.pdf', '.doc', '.docx', '.txt', '.rtf', '.odt', '.md', '.tex',
            # è¡¨æ ¼å’Œæ¼”ç¤ºæ–‡ç¨¿
            '.xls', '.xlsx', '.ppt', '.pptx', '.csv',
            # å›¾åƒ
            '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.tiff',
            # éŸ³é¢‘
            '.mp3', '.wav', '.ogg', '.flac', '.aac',
            # è§†é¢‘
            '.mp4', '.avi', '.mov', '.wmv', '.mkv', '.flv',
            # æ•°æ®å’Œä»£ç 
            '.json', '.xml', '.yaml', '.py', '.js', '.html', '.css', '.java', '.cpp'
        }
        
        try:
            # éå†ç›®å½•
            for root, _, files in os.walk(base_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    file_ext = os.path.splitext(file)[1].lower()
                    
                    # åªå¤„ç†æœ‰æ•ˆæ‰©å±•åçš„æ–‡ä»¶
                    if file_ext in valid_extensions:
                        try:
                            # è·å–æ–‡ä»¶å¤§å°å’Œä¿®æ”¹æ—¶é—´
                            file_stat = os.stat(file_path)
                            
                            # æ”¶é›†æ–‡ä»¶ä¿¡æ¯
                            file_info.append({
                                'name': file,
                                'path': file_path,
                                'extension': file_ext,
                                'size': file_stat.st_size,
                                'modified': datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                                'relative_path': os.path.relpath(file_path, base_dir)
                            })
                        except (PermissionError, FileNotFoundError) as e:
                            logger.warning(f"Error accessing file {file_path}: {e}")
        except Exception as e:
            logger.error(f"Error scanning directory {base_dir}: {e}")
        
        return file_info
    
    @staticmethod
    async def _analyze_and_categorize(file_info: List[Dict]) -> Dict[str, List[Dict]]:
        """åˆ†ææ–‡ä»¶å¹¶ç”Ÿæˆåˆ†ç±»"""
        # å¦‚æœæ–‡ä»¶å¤ªå¤šï¼Œåªå–æ ·æœ¬è¿›è¡Œåˆ†æ
        sample_size = min(100, len(file_info))
        sample_files = random.sample(file_info, sample_size) if len(file_info) > sample_size else file_info
        
        # åŸºäºæ–‡ä»¶æ‰©å±•åçš„ç®€å•åˆ†ç±»
        extension_categories = {
            "æ–‡æ¡£": ['.pdf', '.doc', '.docx', '.txt', '.rtf', '.odt', '.md', '.tex'],
            "è¡¨æ ¼": ['.xls', '.xlsx', '.csv'],
            "æ¼”ç¤ºæ–‡ç¨¿": ['.ppt', '.pptx'],
            "å›¾åƒ": ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.tiff'],
            "éŸ³é¢‘": ['.mp3', '.wav', '.ogg', '.flac', '.aac'],
            "è§†é¢‘": ['.mp4', '.avi', '.mov', '.wmv', '.mkv', '.flv'],
            "æ•°æ®": ['.json', '.xml', '.yaml', '.csv'],
            "ä»£ç ": ['.py', '.js', '.html', '.css', '.java', '.cpp']
        }
        
        # å°è¯•ä½¿ç”¨å¤§æ¨¡å‹APIè¿›è¡Œæ›´æ™ºèƒ½çš„åˆ†ç±»
        try:
            # å‡†å¤‡å‘é€ç»™å¤§æ¨¡å‹çš„æ•°æ®
            prompt_data = {
                "file_sample": sample_files[:20],  # åªå‘é€å‰20ä¸ªæ ·æœ¬
                "current_categories": list(extension_categories.keys())
            }
            
            # è°ƒç”¨å¤§æ¨¡å‹APIè¿›è¡Œåˆ†ç±»
            categories = await ResourceService._call_llm_api({
                "file_sample": sample_files[:20],  # åªå‘é€å‰20ä¸ªæ ·æœ¬
                "current_categories": list(extension_categories.keys()),
                "file_info": file_info  # ä¼ é€’å®Œæ•´çš„æ–‡ä»¶ä¿¡æ¯
            })
            
            # å¦‚æœAPIè°ƒç”¨å¤±è´¥æˆ–è¿”å›ç©ºç»“æœï¼Œä½¿ç”¨åŸºæœ¬åˆ†ç±»
            if not categories:
                categories = ResourceService._basic_categorize(file_info, extension_categories)
            
            # ç¡®ä¿åˆ†ç±»ä¸è¶…è¿‡10ä¸ª
            if len(categories) > 5:
                # åˆå¹¶å°åˆ†ç±»
                sorted_categories = sorted(categories.items(), key=lambda x: len(x[1]), reverse=True)
                top_categories = dict(sorted_categories[:9])  # å–å‰9ä¸ªæœ€å¤§çš„åˆ†ç±»
                
                # å°†å‰©ä½™çš„åˆ†ç±»åˆå¹¶ä¸º"å…¶ä»–"
                other_files = []
                for cat, files in sorted_categories[9:]:
                    other_files.extend(files)
                
                if other_files:
                    top_categories["å…¶ä»–"] = other_files
                
                categories = top_categories
            
            return categories
            
        except Exception as e:
            logger.error(f"Error analyzing files: {e}")
            # å‡ºé”™æ—¶ä½¿ç”¨åŸºæœ¬åˆ†ç±»
            return ResourceService._basic_categorize(file_info, extension_categories)
    
    @staticmethod
    def _basic_categorize(file_info: List[Dict], extension_categories: Dict[str, List[str]]) -> Dict[str, List[Dict]]:
        """åŸºæœ¬çš„æ–‡ä»¶åˆ†ç±»æ–¹æ³•"""
        categories = {}
        
        for file in file_info:
            category = None
            # æ ¹æ®æ‰©å±•ååˆ†ç±»
            for cat, exts in extension_categories.items():
                if file['extension'] in exts:
                    category = cat
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†ç±»ï¼Œå½’ä¸º"å…¶ä»–"
            if not category:
                category = "å…¶ä»–"
            
            # æ·»åŠ åˆ°åˆ†ç±»ä¸­
            if category not in categories:
                categories[category] = []
            categories[category].append(file)
        
        return categories
    
    @staticmethod
    async def _call_llm_api(data: Dict) -> Dict[str, List[Dict]]:
        """è°ƒç”¨å¤§æ¨¡å‹APIè¿›è¡Œæ–‡ä»¶åˆ†ç±»"""
        try:
            # DeepSeek APIé…ç½®
            api_url = "https://api.deepseek.com/v1/chat/completions"  # æ›¿æ¢ä¸ºå®é™…çš„DeepSeek APIåœ°å€
            api_key = os.environ.get("DEEPSEEK_API_KEY")  # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
            
            if not api_key:
                logger.error("DeepSeek API key not found in environment variables")
                return {}
                
            # æ„å»ºæç¤ºè¯
            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹æ–‡ä»¶åˆ—è¡¨ï¼Œå¹¶æ ¹æ®æ–‡ä»¶åå†…å®¹ï¼ˆè€Œéæ–‡ä»¶ç±»å‹ï¼‰å°†å®ƒä»¬åˆ†ç±»åˆ°é€‚å½“çš„ç±»åˆ«ä¸­ã€‚
            è¯·åˆ›å»ºæœ‰æ„ä¹‰çš„ç±»åˆ«ï¼Œè¿™äº›ç±»åˆ«åº”è¯¥åæ˜ æ–‡ä»¶çš„å®é™…å†…å®¹å’Œç”¨é€”ï¼Œè€Œä¸ä»…ä»…æ˜¯æ–‡ä»¶æ ¼å¼ã€‚
            ä¾‹å¦‚ï¼Œ"è´¢åŠ¡æŠ¥è¡¨"ã€"é¡¹ç›®æ–‡æ¡£"ã€"å­¦ä¹ èµ„æ–™"ç­‰ã€‚

            æ–‡ä»¶åˆ—è¡¨:
            {json.dumps(data['file_sample'], ensure_ascii=False, indent=2)}

            è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†ç±»ç»“æœï¼Œæ ¼å¼ä¸º:
            {{
                "ç±»åˆ«1": [æ–‡ä»¶ç´¢å¼•åˆ—è¡¨],
                "ç±»åˆ«2": [æ–‡ä»¶ç´¢å¼•åˆ—è¡¨],
                ...
            }}
            å…¶ä¸­æ–‡ä»¶ç´¢å¼•æ˜¯æ–‡ä»¶åœ¨æä¾›çš„åˆ—è¡¨ä¸­çš„ä½ç½®ï¼ˆä»0å¼€å§‹ï¼‰ã€‚

            è¯·ç¡®ä¿åˆ›å»ºçš„ç±»åˆ«æ•°é‡åœ¨3-7ä¸ªä¹‹é—´ï¼Œå¹¶ä¸”æ¯ä¸ªç±»åˆ«éƒ½æœ‰æ˜ç¡®çš„ä¸»é¢˜ã€‚
            """
            
            # å‡†å¤‡è¯·æ±‚æ•°æ®
            request_data = {
                "model": "deepseek-chat",  # æ›¿æ¢ä¸ºå®é™…çš„DeepSeekæ¨¡å‹åç§°
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»åŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®æ–‡ä»¶åå†…å®¹å¯¹æ–‡ä»¶è¿›è¡Œè¯­ä¹‰åˆ†ç±»ï¼Œè€Œä¸ä»…ä»…æ˜¯æ ¹æ®æ–‡ä»¶æ‰©å±•åã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.2  # ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šæ€§çš„ç»“æœ
            }
            
            # å‘é€è¯·æ±‚åˆ°DeepSeek API
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    api_url,
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {api_key}"
                    },
                    json=request_data
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"DeepSeek API error: {response.status} - {error_text}")
                        return {}
                    
                    result = await response.json()
                    
                    # è§£æAPIè¿”å›çš„ç»“æœ
                    try:
                        content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
                        # æå–JSONéƒ¨åˆ†
                        json_str = content.strip()
                        if "```json" in json_str:
                            json_str = json_str.split("```json")[1].split("```")[0].strip()
                        elif "```" in json_str:
                            json_str = json_str.split("```")[1].split("```")[0].strip()
                        
                        category_indices = json.loads(json_str)
                        
                        # å°†ç´¢å¼•è½¬æ¢ä¸ºå®é™…æ–‡ä»¶
                        categories = {}
                        for category, indices in category_indices.items():
                            categories[category] = []
                            for idx in indices:
                                if 0 <= idx < len(data['file_sample']):
                                    file_info = data['file_sample'][idx]
                                    # æ‰¾åˆ°åŸå§‹æ–‡ä»¶ä¿¡æ¯
                                    for full_file in data.get('file_info', []):
                                        if full_file['path'] == file_info['path']:
                                            categories[category].append(full_file)
                                            break
                        
                        return categories
                    except Exception as e:
                        logger.error(f"Error parsing DeepSeek API response: {e}")
                        logger.debug(f"Raw response: {content}")
                        return {}
        except Exception as e:
            logger.error(f"Error calling DeepSeek API: {e}")
            return {}
    
    @staticmethod
    def _generate_color(category: str) -> str:
        """ä¸ºåˆ†ç±»ç”Ÿæˆä¸€ä¸ªç¨³å®šçš„é¢œè‰²"""
        # ä½¿ç”¨åˆ†ç±»åç§°çš„å“ˆå¸Œå€¼ç”Ÿæˆé¢œè‰²ï¼Œç¡®ä¿åŒä¸€åˆ†ç±»æ€»æ˜¯å¾—åˆ°ç›¸åŒçš„é¢œè‰²
        hash_obj = hashlib.md5(category.encode())
        hash_int = int(hash_obj.hexdigest(), 16)
        
        # ç”ŸæˆæŸ”å’Œçš„é¢œè‰²
        r = (hash_int & 0xFF0000) >> 16
        g = (hash_int & 0x00FF00) >> 8
        b = hash_int & 0x0000FF
        
        # ç¡®ä¿é¢œè‰²ä¸ä¼šå¤ªæš—
        r = max(r, 100)
        g = max(g, 100)
        b = max(b, 100)
        
        return f"#{r:02x}{g:02x}{b:02x}"
    
    @staticmethod
    def _select_icon(category: str) -> str:
        """ä¸ºåˆ†ç±»é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„å›¾æ ‡"""
        # å®šä¹‰åˆ†ç±»å’Œå›¾æ ‡çš„æ˜ å°„
        category_icons = {
            "æ–‡æ¡£": "ğŸ“„",
            "è¡¨æ ¼": "ğŸ“Š",
            "æ¼”ç¤ºæ–‡ç¨¿": "ğŸ“‘",
            "å›¾åƒ": "ğŸ–¼ï¸",
            "éŸ³é¢‘": "ğŸµ",
            "è§†é¢‘": "ğŸ¬",
            "æ•°æ®": "ğŸ“Š",
            "ä»£ç ": "ğŸ’»",
            "å…¶ä»–": "ğŸ“"
        }
        
        # å°è¯•ç²¾ç¡®åŒ¹é…
        if category in category_icons:
            return category_icons[category]
        
        # å°è¯•éƒ¨åˆ†åŒ¹é…
        for key, icon in category_icons.items():
            if key in category or category in key:
                return icon
        
        # é»˜è®¤å›¾æ ‡
        return "ğŸ“"

    @staticmethod
    async def start_analysis_task(base_dir: str, file_list=None, options=None) -> str:
        """å¯åŠ¨å¼‚æ­¥åˆ†æä»»åŠ¡
        
        Args:
            base_dir: åŸºç¡€ç›®å½•è·¯å¾„
            file_list: æ–‡ä»¶åˆ—è¡¨ï¼Œå¦‚æœæä¾›åˆ™åªåˆ†æè¿™äº›æ–‡ä»¶
            options: å…¶ä»–é€‰é¡¹
        
        Returns:
            str: ä»»åŠ¡ID
        """
        # ç”Ÿæˆå”¯ä¸€ä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # å¦‚æœç›®å½•ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ç›®å½•
        if not base_dir or not os.path.exists(base_dir):
            base_dir = os.path.join(os.path.expanduser("~"), "Documents")
        
        # åˆ›å»ºä»»åŠ¡å¯¹è±¡
        task = AnalysisTask(task_id)
        # ä½¿ç”¨ç±»å˜é‡å­˜å‚¨ä»»åŠ¡
        ResourceService._analysis_tasks[task_id] = task
        
        # å¯åŠ¨å¼‚æ­¥ä»»åŠ¡
        asyncio.create_task(ResourceService._run_analysis_task(task, base_dir, file_list, options))
        
        return task_id

    @staticmethod
    async def get_task_status(task_id: str) -> Dict:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        logger.info(f"Getting task status for task_id: {task_id}")
        logger.info(f"Available tasks: {list(ResourceService._analysis_tasks.keys())}")
        
        task = ResourceService._analysis_tasks.get(task_id)
        if not task:
            logger.warning(f"Task not found: {task_id}")
            return {"status": "not_found"}
        
        result = {
            "status": task.status,
            "progress": task.progress
        }
        
        # å¦‚æœä»»åŠ¡å®Œæˆï¼Œè¿”å›ç»“æœ
        if task.status == "completed":
            result["result"] = task.result
        elif task.status == "failed":
            result["error"] = str(task.error)
        
        # è®¡ç®—è¿è¡Œæ—¶é—´
        result["elapsed_time"] = time.time() - task.start_time
        
        logger.info(f"Task status: {result}")
        return result

    @staticmethod
    async def _run_analysis_task(task: AnalysisTask, base_dir: str, file_list=None, options=None):
        """è¿è¡Œåˆ†æä»»åŠ¡"""
        try:
            task.status = "running"
            task.progress = 5
            
            # å¦‚æœæä¾›äº†æ–‡ä»¶åˆ—è¡¨ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
            if file_list:
                logger.info(f"Using provided file list with {len(file_list)} files")
                
                # æ”¶é›†æ–‡ä»¶å¤¹ä¿¡æ¯
                folder_structure = {}
                
                # å¤„ç†æ–‡ä»¶åˆ—è¡¨ï¼ŒæŒ‰æ–‡ä»¶å¤¹åˆ†ç»„
                for file in file_list:
                    path = file["path"]
                    # æå–æ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„
                    folder_path = os.path.dirname(path)
                    
                    # å¦‚æœæ˜¯æ ¹ç›®å½•ï¼Œä½¿ç”¨ç‰¹æ®Šæ ‡è®°
                    if not folder_path:
                        folder_path = "æ ¹ç›®å½•"
                    
                    # å°†æ–‡ä»¶æ·»åŠ åˆ°å¯¹åº”çš„æ–‡ä»¶å¤¹
                    if folder_path not in folder_structure:
                        folder_structure[folder_path] = []
                    
                    # æå–æ–‡ä»¶æ‰©å±•å
                    file_ext = os.path.splitext(file["name"])[1].lower()
                    
                    folder_structure[folder_path].append({
                        'name': file["name"],
                        'path': path,
                        'extension': file_ext,
                        'size': 0,  # å‰ç«¯æ— æ³•è·å–æ–‡ä»¶å¤§å°
                        'modified': datetime.now().isoformat(),
                        'relative_path': path
                    })
                
                # ç›´æ¥ä½¿ç”¨æ–‡ä»¶å¤¹åç§°ä½œä¸ºåˆ†ç±»
                categories = {}
                for folder_path, files in folder_structure.items():
                    # è·å–æ–‡ä»¶å¤¹åç§°ï¼ˆæœ€åä¸€çº§ç›®å½•åï¼‰
                    folder_name = os.path.basename(folder_path)
                    if not folder_name:
                        folder_name = os.path.basename(os.path.dirname(folder_path))
                    if not folder_name:
                        folder_name = "å…¶ä»–"
                    
                    # ä½¿ç”¨æ–‡ä»¶å¤¹åç§°ä½œä¸ºåˆ†ç±»
                    if folder_name not in categories:
                        categories[folder_name] = []
                    
                    categories[folder_name].extend(files)
                
                task.progress = 90
            
            else:
                # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œæ–‡ä»¶ç³»ç»Ÿæ‰«æ
                with ThreadPoolExecutor(max_workers=min(32, (os.cpu_count() or 1) * 4)) as executor:
                    # é¦–å…ˆè·å–é¡¶å±‚ç›®å½•åˆ—è¡¨
                    try:
                        top_dirs = [os.path.join(base_dir, d) for d in os.listdir(base_dir) 
                                   if os.path.isdir(os.path.join(base_dir, d))]
                        # æ·»åŠ åŸºç›®å½•æœ¬èº«
                        top_dirs.append(base_dir)
                    except Exception as e:
                        logger.error(f"Error listing directory {base_dir}: {e}")
                        top_dirs = [base_dir]
                    
                    # ä¸ºæ¯ä¸ªé¡¶å±‚ç›®å½•åˆ›å»ºä¸€ä¸ªæ‰«æä»»åŠ¡
                    scan_tasks = []
                    for directory in top_dirs:
                        scan_tasks.append(
                            asyncio.get_event_loop().run_in_executor(
                                executor,
                                ResourceService._scan_directory,
                                directory,
                                base_dir
                            )
                        )
                    
                    # ä½¿ç”¨asyncio.as_completedå¤„ç†ç»“æœï¼Œæ›´æ–°è¿›åº¦
                    file_info = []
                    completed = 0
                    for future in asyncio.as_completed(scan_tasks):
                        chunk_result = await future
                        file_info.extend(chunk_result)
                        
                        # æ›´æ–°è¿›åº¦ï¼ˆä»5%åˆ°50%ï¼‰
                        completed += 1
                        progress = 5 + int((completed / len(scan_tasks)) * 45)
                        task.progress = progress
                    
                    # æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„
                    folder_structure = {}
                    for file in file_info:
                        # æå–æ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„
                        folder_path = os.path.dirname(file['path'])
                        
                        # å°†æ–‡ä»¶æ·»åŠ åˆ°å¯¹åº”çš„æ–‡ä»¶å¤¹
                        if folder_path not in folder_structure:
                            folder_structure[folder_path] = []
                        
                        folder_structure[folder_path].append(file)
                    
                    # ç›´æ¥ä½¿ç”¨æ–‡ä»¶å¤¹åç§°ä½œä¸ºåˆ†ç±»
                    categories = {}
                    for folder_path, files in folder_structure.items():
                        # è·å–æ–‡ä»¶å¤¹åç§°ï¼ˆæœ€åä¸€çº§ç›®å½•åï¼‰
                        folder_name = os.path.basename(folder_path)
                        if not folder_name:
                            folder_name = "å…¶ä»–"
                        
                        # ä½¿ç”¨æ–‡ä»¶å¤¹åç§°ä½œä¸ºåˆ†ç±»
                        if folder_name not in categories:
                            categories[folder_name] = []
                        
                        categories[folder_name].extend(files)
                
                task.progress = 90
            
            # 5. æ„å»ºæœ€ç»ˆç»“æœ
            result = []
            for category, files in categories.items():
                result.append({
                    "id": len(result) + 1,
                    "name": category,
                    "count": len(files),
                    "icon": ResourceService._select_icon(category),
                    "color": ResourceService._generate_color(category),
                    "files": files
                })
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = "completed"
            task.progress = 100
            task.result = result
            
            # æ›´æ–°ç¼“å­˜
            ResourceService._cache = result
            ResourceService._cache_time = datetime.now()
            
        except Exception as e:
            logger.error(f"Analysis task failed: {e}")
            task.status = "failed"
            task.error = e

    @staticmethod
    def _scan_directory(directory: str, base_dir: str) -> List[Dict]:
        """æ‰«æå•ä¸ªç›®å½•åŠå…¶å­ç›®å½•ä¸­çš„æ–‡ä»¶ï¼ˆç”¨äºçº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œï¼‰"""
        file_info = []
        
        # å®šä¹‰è¦æ”¶é›†çš„æ–‡ä»¶æ‰©å±•å
        valid_extensions = {
            # æ–‡æ¡£
            '.pdf', '.doc', '.docx', '.txt', '.rtf', '.odt', '.md', '.tex',
            # è¡¨æ ¼å’Œæ¼”ç¤ºæ–‡ç¨¿
            '.xls', '.xlsx', '.ppt', '.pptx', '.csv',
            # å›¾åƒ
            '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.tiff',
            # éŸ³é¢‘
            '.mp3', '.wav', '.ogg', '.flac', '.aac',
            # è§†é¢‘
            '.mp4', '.avi', '.mov', '.wmv', '.mkv', '.flv',
            # æ•°æ®å’Œä»£ç 
            '.json', '.xml', '.yaml', '.py', '.js', '.html', '.css', '.java', '.cpp'
        }
        
        try:
            # ä½¿ç”¨os.scandirä»£æ›¿os.walkä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
            for entry in os.scandir(directory):
                try:
                    if entry.is_file():
                        file_ext = os.path.splitext(entry.name)[1].lower()
                        
                        # åªå¤„ç†æœ‰æ•ˆæ‰©å±•åçš„æ–‡ä»¶
                        if file_ext in valid_extensions:
                            try:
                                # è·å–æ–‡ä»¶å¤§å°å’Œä¿®æ”¹æ—¶é—´
                                file_stat = entry.stat()
                                
                                # æ”¶é›†æ–‡ä»¶ä¿¡æ¯
                                file_path = entry.path
                                file_info.append({
                                    'name': entry.name,
                                    'path': file_path,
                                    'extension': file_ext,
                                    'size': file_stat.st_size,
                                    'modified': datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                                    'relative_path': os.path.relpath(file_path, base_dir)
                                })
                            except (PermissionError, FileNotFoundError) as e:
                                logger.warning(f"Error accessing file {entry.path}: {e}")
                    
                    # é€’å½’å¤„ç†å­ç›®å½•
                    elif entry.is_dir():
                        # è·³è¿‡éšè—ç›®å½•
                        if entry.name.startswith('.'):
                            continue
                        
                        # é€’å½’æ‰«æå­ç›®å½•
                        sub_files = ResourceService._scan_directory(entry.path, base_dir)
                        file_info.extend(sub_files)
                        
                except (PermissionError, FileNotFoundError) as e:
                    logger.warning(f"Error accessing entry {entry.path}: {e}")
                    
        except Exception as e:
            logger.error(f"Error scanning directory {directory}: {e}")
        
        return file_info

    @staticmethod
    def _get_extension_categories() -> Dict[str, List[str]]:
        """è·å–æ–‡ä»¶æ‰©å±•ååˆ†ç»„ï¼ˆä»…ç”¨äºå†…éƒ¨å¤„ç†ï¼Œä¸ä½œä¸ºæœ€ç»ˆåˆ†ç±»åç§°ï¼‰"""
        # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯å°†æ‰©å±•ååˆ†ç»„ï¼Œä¸æ˜¯æœ€ç»ˆçš„åˆ†ç±»åç§°
        # æœ€ç»ˆåˆ†ç±»åç§°å°†ç”±æ–‡ä»¶å¤¹åç§°å’Œå†…å®¹æ™ºèƒ½ç”Ÿæˆ
        return {
            "text_files": ['.pdf', '.doc', '.docx', '.txt', '.rtf', '.odt', '.md', '.tex'],
            "spreadsheets": ['.xls', '.xlsx', '.csv'],
            "presentations": ['.ppt', '.pptx'],
            "images": ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.tiff'],
            "audio": ['.mp3', '.wav', '.ogg', '.flac', '.aac'],
            "video": ['.mp4', '.avi', '.mov', '.wmv', '.mkv', '.flv'],
            "data": ['.json', '.xml', '.yaml', '.csv'],
            "code": ['.py', '.js', '.html', '.css', '.java', '.cpp']
        }

    @staticmethod
    def _basic_categorize(file_info: List[Dict], extension_categories: Dict[str, List[str]]) -> Dict[str, List[Dict]]:
        """åŸºäºæ–‡ä»¶å¤¹ç»„ç»‡çš„åˆ†ç±»æ–¹æ³•ï¼ˆå½“APIè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        # æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„
        folder_structure = {}
        for file in file_info:
            # æå–æ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„
            folder_path = os.path.dirname(file['path'])
            
            # å°†æ–‡ä»¶æ·»åŠ åˆ°å¯¹åº”çš„æ–‡ä»¶å¤¹
            if folder_path not in folder_structure:
                folder_structure[folder_path] = []
            
            folder_structure[folder_path].append(file)
        
        # ç›´æ¥ä½¿ç”¨æ–‡ä»¶å¤¹åç§°ä½œä¸ºåˆ†ç±»
        categories = {}
        for folder_path, files in folder_structure.items():
            # è·å–æ–‡ä»¶å¤¹åç§°ï¼ˆæœ€åä¸€çº§ç›®å½•åï¼‰
            folder_name = os.path.basename(folder_path)
            if not folder_name:
                folder_name = os.path.basename(os.path.dirname(folder_path))
            if not folder_name:
                folder_name = "æœªåˆ†ç±»æ–‡ä»¶"
            
            # ä½¿ç”¨æ–‡ä»¶å¤¹åç§°ä½œä¸ºåˆ†ç±»
            if folder_name not in categories:
                categories[folder_name] = []
            
            categories[folder_name].extend(files)
        
        return categories

    @staticmethod
    async def auto_analyze_local_directories():
        """è‡ªåŠ¨åˆ†ææœ¬åœ°æ–‡ä»¶å¤¹"""
        # é˜²æ­¢é‡å¤è¿è¡Œ
        if ResourceService._auto_analysis_running:
            logger.info("Auto analysis already running, skipping")
            return
        
        try:
            ResourceService._auto_analysis_running = True
            logger.info("Starting automatic analysis of local directories")
            
            # è·å–ç”¨æˆ·ä¸»ç›®å½•
            home_dir = os.path.expanduser("~")
            common_dirs = [
                os.path.join(home_dir, "Documents"),
                os.path.join(home_dir, "Downloads"),
                os.path.join(home_dir, "Pictures"),
                os.path.join(home_dir, "Music"),
                os.path.join(home_dir, "Videos"),
                os.path.join(home_dir, "Desktop")
            ]
            
            # æ”¶é›†å­˜åœ¨çš„ç›®å½•
            existing_dirs = [d for d in common_dirs if os.path.exists(d) and os.path.isdir(d)]
            print('existing_dirs',existing_dirs)
            # åˆ›å»ºåˆ†æä»»åŠ¡
            task_id = str(uuid.uuid4())
            task = AnalysisTask(task_id)
            ResourceService._analysis_tasks[task_id] = task
            
            # è¿è¡Œåˆ†æä»»åŠ¡
            await ResourceService._run_auto_analysis(task, existing_dirs)
            
            # å­˜å‚¨ç»“æœ
            if task.status == "completed":
                # ç¼“å­˜åˆ†æç»“æœ
                ResourceService._auto_analysis_result = task.result
                ResourceService._auto_analysis_time = datetime.now()
                
                # å°†ç»“æœä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œä»¥ä¾¿æœåŠ¡é‡å¯åä»èƒ½ä½¿ç”¨
                try:
                    cache_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "cache")
                    os.makedirs(cache_dir, exist_ok=True)
                    
                    cache_file = os.path.join(cache_dir, "auto_analysis_cache.json")
                    with open(cache_file, "w", encoding="utf-8") as f:
                        json.dump({
                            "result": task.result,
                            "timestamp": datetime.now().isoformat()
                        }, f, ensure_ascii=False, indent=2)
                    
                    logger.info(f"Cached analysis result to {cache_file}")
                except Exception as e:
                    logger.error(f"Failed to cache analysis result: {e}")
                
                logger.info(f"Automatic analysis completed with {len(task.result)} categories")
            else:
                logger.error(f"Automatic analysis failed: {task.error}")
        
        except Exception as e:
            logger.error(f"Error in automatic analysis: {e}")
        finally:
            ResourceService._auto_analysis_running = False

    @staticmethod
    async def _run_auto_analysis(task: AnalysisTask, directories: List[str]):
        """è¿è¡Œè‡ªåŠ¨åˆ†æä»»åŠ¡ - å®Œå…¨åŸºäºæ–‡ä»¶å¤¹å†…å®¹å’Œåç§°è¿›è¡Œæ™ºèƒ½åˆ†ç±»"""
        try:
            task.status = "running"
            task.progress = 5
            
            # å®Œå…¨åŸºäºæ–‡ä»¶å¤¹å†…å®¹å’Œåç§°è¿›è¡Œæ™ºèƒ½åˆ†ç±»
            result = []
            
            for i, directory in enumerate(directories):
                try:
                    # è·å–ç›®å½•åç§°
                    dir_name = os.path.basename(directory)
                    
                    # è®¡ç®—ç›®å½•ä¸­çš„æ–‡ä»¶æ•°é‡å’Œç±»å‹
                    total_files = 0
                    file_list = []
                    extensions = {}
                    file_names = []
                    
                    # éå†ç›®å½•ä¸­çš„æ–‡ä»¶
                    for root, _, files in os.walk(directory):
                        for file in files:
                            try:
                                file_path = os.path.join(root, file)
                                # è·å–æ–‡ä»¶æ‰©å±•å
                                ext = os.path.splitext(file)[1].lower()
                                
                                # ç»Ÿè®¡æ‰©å±•å
                                if ext:
                                    extensions[ext] = extensions.get(ext, 0) + 1
                                
                                # æ”¶é›†æ–‡ä»¶åç”¨äºåç»­åˆ†æ
                                file_names.append(file.lower())
                                
                                # è·å–ç›¸å¯¹è·¯å¾„
                                rel_path = os.path.relpath(file_path, directory)
                                
                                # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
                                file_list.append({
                                    'name': file,
                                    'path': file_path,
                                    'extension': ext,
                                    'size': os.path.getsize(file_path) if os.path.exists(file_path) else 0,
                                    'modified': datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat() if os.path.exists(file_path) else "",
                                    'relative_path': rel_path
                                })
                                
                                total_files += 1
                            except Exception as e:
                                logger.warning(f"Error processing file {file}: {e}")
                                continue
                    
                    # æ™ºèƒ½åˆ†ææ–‡ä»¶å¤¹å†…å®¹ï¼Œç”Ÿæˆæè¿°æ€§åˆ†ç±»åç§°
                    category_name = ResourceService._analyze_folder_content(dir_name, file_names, extensions)
                    
                    # æ·»åŠ ç›®å½•ä½œä¸ºä¸€ä¸ªåˆ†ç±»
                    result.append({
                        "id": i + 1,
                        "name": category_name,
                        "count": total_files,
                        "icon": "ğŸ“",  # ä½¿ç”¨é€šç”¨å›¾æ ‡
                        "color": ResourceService._generate_color(category_name),
                        "files": file_list[:100]  # é™åˆ¶æ–‡ä»¶æ•°é‡ï¼Œé¿å…è¿”å›è¿‡å¤šæ•°æ®
                    })
                    
                    # æ›´æ–°è¿›åº¦
                    task.progress = 5 + int((i + 1) / len(directories) * 85)
                except Exception as e:
                    logger.error(f"Error processing directory {directory}: {e}")
                    continue
            print('result',result)
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = "completed"
            task.progress = 100
            task.result = result
        except Exception as e:
            logger.error(f"Auto analysis task failed: {e}")
            task.status = "failed"
            task.error = str(e)

    @staticmethod
    def _analyze_folder_content(folder_name, file_names, extensions):
        """æ™ºèƒ½åˆ†ææ–‡ä»¶å¤¹å†…å®¹ï¼Œç”Ÿæˆæè¿°æ€§åˆ†ç±»åç§°"""
        # æ¸…ç†æ–‡ä»¶å¤¹åç§°ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦å¹¶è½¬æ¢ä¸ºæ›´å¯è¯»çš„æ ¼å¼
        clean_name = folder_name.replace("_", " ").replace("-", " ").strip()
        
        # å¦‚æœæ–‡ä»¶å¤¹åç§°å·²ç»å¾ˆæœ‰æè¿°æ€§ï¼Œç›´æ¥ä½¿ç”¨
        if len(clean_name) > 3 and not clean_name.lower() in ["documents", "downloads", "pictures", "music", "videos", "desktop", "æ–‡æ¡£", "ä¸‹è½½", "å›¾ç‰‡", "éŸ³ä¹", "è§†é¢‘", "æ¡Œé¢"]:
            return clean_name
        
        # åˆ†ææ–‡ä»¶æ‰©å±•ååˆ†å¸ƒ
        total_files = sum(extensions.values())
        if total_files == 0:
            return clean_name
        
        # åˆ†ææ–‡ä»¶åä¸­çš„å¸¸è§è¯æ±‡
        common_words = {}
        for name in file_names:
            # åˆ†å‰²æ–‡ä»¶åä¸ºå•è¯
            words = re.findall(r'[a-zA-Z\u4e00-\u9fa5]+', name)
            for word in words:
                if len(word) > 1:  # å¿½ç•¥å•ä¸ªå­—ç¬¦
                    word = word.lower()
                    common_words[word] = common_words.get(word, 0) + 1
        
        # ç§»é™¤å¸¸è§çš„æ— æ„ä¹‰è¯æ±‡
        stop_words = ["the", "a", "an", "and", "or", "of", "to", "in", "for", "with", "on", "at", "from", "by", "about", "as", "file", "doc", "document", "image", "photo", "picture", "video", "audio", "music", "data", "code", "program", "script", "text", "pdf", "word", "excel", "powerpoint", "ppt", "xls", "doc", "docx", "xlsx", "pptx", "txt", "jpg", "png", "mp3", "mp4", "avi", "mov", "wav", "ogg", "flac", "zip", "rar", "7z", "tar", "gz", "çš„", "äº†", "å’Œ", "ä¸", "æˆ–", "åœ¨", "æ˜¯", "æœ‰", "æ–‡ä»¶", "å›¾ç‰‡", "è§†é¢‘", "éŸ³é¢‘", "æ•°æ®", "ä»£ç ", "æ–‡æœ¬"]
        for word in stop_words:
            if word in common_words:
                del common_words[word]
        
        # æ‰¾å‡ºæœ€å¸¸è§çš„è¯æ±‡
        if common_words:
            sorted_words = sorted(common_words.items(), key=lambda x: x[1], reverse=True)
            top_words = [word for word, count in sorted_words[:3] if count > 2]
            
            if top_words:
                # æ ¹æ®æœ€å¸¸è§çš„è¯æ±‡ç”Ÿæˆåˆ†ç±»åç§°
                if len(top_words) == 1:
                    return top_words[0].capitalize() + "ç›¸å…³æ–‡ä»¶"
                else:
                    return "ä¸" + "ã€".join(top_words) + "ç›¸å…³çš„æ–‡ä»¶"
        
        # åˆ†ææ–‡ä»¶ç±»å‹åˆ†å¸ƒ
        doc_exts = ['.pdf', '.doc', '.docx', '.txt', '.md']
        img_exts = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']
        code_exts = ['.py', '.js', '.html', '.css', '.java', '.cpp']
        data_exts = ['.csv', '.json', '.xml', '.xlsx']
        audio_exts = ['.mp3', '.wav', '.ogg', '.flac', '.aac']
        video_exts = ['.mp4', '.avi', '.mov', '.wmv', '.mkv', '.flv']
        
        # è®¡ç®—å„ç±»å‹æ–‡ä»¶çš„æ¯”ä¾‹
        doc_ratio = sum(extensions.get(ext, 0) for ext in doc_exts) / total_files
        img_ratio = sum(extensions.get(ext, 0) for ext in img_exts) / total_files
        code_ratio = sum(extensions.get(ext, 0) for ext in code_exts) / total_files
        data_ratio = sum(extensions.get(ext, 0) for ext in data_exts) / total_files
        audio_ratio = sum(extensions.get(ext, 0) for ext in audio_exts) / total_files
        video_ratio = sum(extensions.get(ext, 0) for ext in video_exts) / total_files
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹åˆ†å¸ƒç”Ÿæˆæ›´å…·æè¿°æ€§çš„åç§°
        if doc_ratio > 0.7:
            return folder_name + "æ–‡æ¡£é›†åˆ"
        elif img_ratio > 0.7:
            return folder_name + "å›¾åƒé›†åˆ"
        elif code_ratio > 0.7:
            return folder_name + "ä»£ç é¡¹ç›®"
        elif data_ratio > 0.7:
            return folder_name + "æ•°æ®é›†åˆ"
        elif audio_ratio > 0.7:
            return folder_name + "éŸ³é¢‘é›†åˆ"
        elif video_ratio > 0.7:
            return folder_name + "è§†é¢‘é›†åˆ"
        elif doc_ratio + img_ratio > 0.7:
            return folder_name + "æ–‡æ¡£ä¸å›¾åƒé›†åˆ"
        elif code_ratio + data_ratio > 0.7:
            return folder_name + "ä»£ç ä¸æ•°æ®é¡¹ç›®"
        
        # å¦‚æœæ— æ³•ç¡®å®šæ˜ç¡®çš„åˆ†ç±»ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶å¤¹åç§°
        return clean_name

    @staticmethod
    async def get_auto_analysis_result():
        """è·å–è‡ªåŠ¨åˆ†æç»“æœ"""
        # å¦‚æœå†…å­˜ä¸­æœ‰ç»“æœä¸”æœªè¿‡æœŸï¼Œç›´æ¥è¿”å›
        if (ResourceService._auto_analysis_result is not None and 
            ResourceService._auto_analysis_time is not None and
            datetime.now() - ResourceService._auto_analysis_time < timedelta(hours=24)):
            logger.info("Using in-memory auto analysis result")
            return ResourceService._auto_analysis_result
        
        # å¦‚æœå†…å­˜ä¸­æ²¡æœ‰ç»“æœæˆ–å·²è¿‡æœŸï¼Œå°è¯•ä»ç¼“å­˜æ–‡ä»¶åŠ è½½
        try:
            cache_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), "cache", "auto_analysis_cache.json")
            if os.path.exists(cache_file):
                with open(cache_file, "r", encoding="utf-8") as f:
                    cache_data = json.load(f)
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
                cache_time = datetime.fromisoformat(cache_data["timestamp"])
                if datetime.now() - cache_time < timedelta(hours=24):
                    logger.info("Loaded auto analysis result from cache file")
                    ResourceService._auto_analysis_result = cache_data["result"]
                    ResourceService._auto_analysis_time = cache_time
                    return ResourceService._auto_analysis_result
        except Exception as e:
            logger.error(f"Failed to load cached analysis result: {e}")
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ç¼“å­˜ï¼Œé‡æ–°åˆ†æ
        logger.info("No valid cache found, starting new auto analysis")
        await ResourceService.auto_analyze_local_directories()
        
        return ResourceService._auto_analysis_result

    @staticmethod
    async def get_cached_analysis_result():
        """åªè·å–ç¼“å­˜çš„åˆ†æç»“æœï¼Œä¸è§¦å‘æ–°çš„åˆ†æ"""
        # æ¸…ç©ºå†…å­˜ç¼“å­˜
        ResourceService._auto_analysis_result = None
        ResourceService._auto_analysis_time = None
        
        # æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
        try:
            cache_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), "cache", "auto_analysis_cache.json")
            if os.path.exists(cache_file):
                # æ¸…ç©ºæ–‡ä»¶ç¼“å­˜
                os.remove(cache_file)
                logger.info(f"Removed cache file: {cache_file}")
                return None
        except Exception as e:
            logger.error(f"Failed to remove cached analysis result: {e}")
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ç¼“å­˜ï¼Œè¿”å›None
        return None






